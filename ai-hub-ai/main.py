"""
AI Hub Excel å¯¼å…¥æœåŠ¡
FastAPI æœåŠ¡ï¼Œç”¨äºå¤„ç† Excel å¯¼å…¥å¹¶è°ƒç”¨ .NET åç«¯ API
"""
import os
import traceback
from typing import List, Optional
from io import BytesIO
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import pandas as pd
import httpx
from pathlib import Path
import json
import re
import logging
from urllib.parse import quote

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="AI Hub Excel å¯¼å…¥æœåŠ¡",
    description="å¤„ç† Excel æ–‡ä»¶å¯¼å…¥ï¼Œè½¬æ¢ä¸ºçŸ¥è¯†æ¡ç›®",
    version="1.0.0"
)

# é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
DOTNET_BASE_URL = os.getenv("DOTNET_BASE_URL", "http://localhost:5000")
INTERNAL_TOKEN = os.getenv("INTERNAL_TOKEN", "your-internal-token-change-in-production")
DEFAULT_TENANT = os.getenv("DEFAULT_TENANT", "default")
ATTACHMENT_BASE_PATH = os.getenv("ATTACHMENT_BASE_PATH", "")
ATTACHMENT_BASE_URL = os.getenv("ATTACHMENT_BASE_URL", "http://localhost:5000/uploads")

# åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from dotenv import load_dotenv
    load_dotenv()
    # é‡æ–°è¯»å–ç¯å¢ƒå˜é‡ï¼ˆ.env æ–‡ä»¶ä¸­çš„å€¼ä¼šè¦†ç›–ä¸Šé¢çš„é»˜è®¤å€¼ï¼‰
    DOTNET_BASE_URL = os.getenv("DOTNET_BASE_URL", DOTNET_BASE_URL)
    INTERNAL_TOKEN = os.getenv("INTERNAL_TOKEN", INTERNAL_TOKEN)
    DEFAULT_TENANT = os.getenv("DEFAULT_TENANT", DEFAULT_TENANT)
    ATTACHMENT_BASE_PATH = os.getenv("ATTACHMENT_BASE_PATH", ATTACHMENT_BASE_PATH)
    ATTACHMENT_BASE_URL = os.getenv("ATTACHMENT_BASE_URL", ATTACHMENT_BASE_URL)
    logger.info(f"é…ç½®åŠ è½½å®Œæˆ: DOTNET_BASE_URL={DOTNET_BASE_URL}, DEFAULT_TENANT={DEFAULT_TENANT}, ATTACHMENT_BASE_PATH={ATTACHMENT_BASE_PATH}")
except ImportError:
    logger.warning("python-dotenv æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼")


class ExcelImportResponse(BaseModel):
    """Excel å¯¼å…¥å“åº”"""
    total_rows: int
    success_count: int
    failure_count: int
    article_ids: List[int]
    failures: List[dict]


class ExcelRowFailure(BaseModel):
    """Excel è¡Œå¤„ç†å¤±è´¥ä¿¡æ¯"""
    row_index: int
    reason: str


def format_as_reasons(text: Optional[str]) -> Optional[str]:
    """æ ¼å¼åŒ–ä¸ºåŸå› åˆ—è¡¨ï¼ˆåŸå›  1ï¼š...ï¼‰"""
    if not text or not text.strip():
        return None
    
    # æŒ‰æ¢è¡Œç¬¦æˆ–åˆ†å·åˆ†å‰²
    reasons = [r.strip() for r in re.split(r'[\n\rï¼›;]', text) if r.strip()]
    
    if not reasons:
        return None
    
    # å¦‚æœå·²ç»æ˜¯"åŸå›  Xï¼š"æ ¼å¼ï¼Œç›´æ¥è¿”å›
    if any(re.match(r'^åŸå› \s*\d+[ï¼š:]', r) for r in reasons):
        return '\n'.join(reasons)
    
    # å¦åˆ™æ·»åŠ ç¼–å·
    return '\n'.join([f"åŸå›  {i + 1}ï¼š{r}" for i, r in enumerate(reasons)])


def format_as_steps(text: Optional[str]) -> Optional[str]:
    """æ ¼å¼åŒ–ä¸ºæ­¥éª¤åˆ—è¡¨ï¼ˆæ­¥éª¤ 1ï¼š...ï¼‰"""
    if not text or not text.strip():
        return None
    
    # æŒ‰æ¢è¡Œç¬¦æˆ–åˆ†å·åˆ†å‰²
    steps = [s.strip() for s in re.split(r'[\n\rï¼›;]', text) if s.strip()]
    
    if not steps:
        return None
    
    # å¦‚æœå·²ç»æ˜¯"æ­¥éª¤ Xï¼š"æ ¼å¼ï¼Œç›´æ¥è¿”å›
    if any(re.match(r'^æ­¥éª¤\s*\d+[ï¼š:]', s) for s in steps):
        return '\n'.join(steps)
    
    # å¦åˆ™æ·»åŠ ç¼–å·
    return '\n'.join([f"æ­¥éª¤ {i + 1}ï¼š{s}" for i, s in enumerate(steps)])


def extract_filename_from_reference(text: str) -> Optional[str]:
    """
    ä»æ–‡æœ¬å¼•ç”¨ä¸­æå–æ–‡ä»¶å
    æ”¯æŒæ ¼å¼ï¼š
    - å‚è€ƒ"xxx" æˆ– å‚è€ƒ"xxx"ï¼ˆä¸­æ–‡å¼•å·ï¼‰
    - å‚è€ƒï¼šxxx
    - è§é™„ä»¶ï¼šxxx
    - å‚è€ƒ xxx
    """
    if not text or not text.strip():
        return None
    
    text = text.strip()
    
    # åŒ¹é…ï¼šå‚è€ƒ"xxx" æˆ– å‚è€ƒ"xxx"ï¼ˆæ”¯æŒä¸­æ–‡å¼•å· "" å’Œè‹±æ–‡å¼•å· ""ï¼‰
    # åŒ¹é…ï¼šå‚è€ƒ"xxx"ã€å‚è€ƒ"xxx"ã€å‚è€ƒ"xxx"
    match = re.search(r'å‚è€ƒ["""""]([^"""""]+)["""""]', text)
    if match:
        filename = match.group(1).strip()
        # å»é™¤å¯èƒ½æ®‹ç•™çš„å¼•å·
        filename = filename.strip('"""\'"')
        return filename if filename else None
    
    # åŒ¹é…ï¼šå‚è€ƒï¼šxxx æˆ– å‚è€ƒ:xxx
    match = re.search(r'å‚è€ƒ[ï¼š:]\s*(.+)', text)
    if match:
        filename = match.group(1).strip()
        # å»é™¤å¯èƒ½çš„å‰åå¼•å·
        filename = filename.strip('"""\'"')
        return filename if filename else None
    
    # åŒ¹é…ï¼šè§é™„ä»¶ï¼šxxx
    match = re.search(r'è§é™„ä»¶[ï¼š:]\s*(.+)', text)
    if match:
        filename = match.group(1).strip()
        filename = filename.strip('"""\'"')
        return filename if filename else None
    
    # åŒ¹é…ï¼šå‚è€ƒ xxxï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
    match = re.search(r'å‚è€ƒ\s+(.+)', text)
    if match:
        filename = match.group(1).strip()
        filename = filename.strip('"""\'"')
        return filename if filename else None
    
    # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œè¿”å›åŸæ–‡ï¼ˆå»é™¤"å‚è€ƒ"ç­‰å‰ç¼€ï¼‰
    filename = re.sub(r'^(å‚è€ƒ|è§é™„ä»¶)[ï¼š:\s]*', '', text)
    filename = filename.strip('"""\'"')
    return filename.strip() if filename.strip() else None


def find_attachment_file(filename: str) -> Optional[dict]:
    """
    åœ¨å›ºå®šç›®å½•ä¸­é€’å½’æŸ¥æ‰¾é™„ä»¶æ–‡ä»¶ï¼ˆæ”¯æŒæ–‡ä»¶å¤¹åµŒå¥—ï¼‰
    è¿”å›ï¼š{ "path": æ–‡ä»¶è·¯å¾„, "url": è®¿é—®URL, "type": æ–‡ä»¶ç±»å‹, "size": æ–‡ä»¶å¤§å°, "file_name": æ–‡ä»¶å }
    """
    if not ATTACHMENT_BASE_PATH or not ATTACHMENT_BASE_PATH.strip():
        logger.debug(f"ATTACHMENT_BASE_PATH æœªé…ç½®ï¼Œè·³è¿‡æ–‡ä»¶æŸ¥æ‰¾: {filename}")
        return None
    
    base_path = Path(ATTACHMENT_BASE_PATH.strip())
    if not base_path.exists():
        logger.debug(f"é™„ä»¶åŸºç¡€è·¯å¾„ä¸å­˜åœ¨: {base_path}ï¼Œè·³è¿‡æ–‡ä»¶æŸ¥æ‰¾: {filename}")
        return None
    
    # æ¸…ç†æ–‡ä»¶åï¼šå»é™¤æ‰€æœ‰ç±»å‹çš„å¼•å·å’Œå‰åç©ºæ ¼
    clean_filename = filename.strip()
    # å»é™¤å„ç§å¼•å·ï¼ˆä¸­æ–‡å¼•å·ã€è‹±æ–‡å¼•å·ï¼‰
    clean_filename = clean_filename.strip('"""\'"ã€Šã€‹ã€ã€‘[]()ï¼ˆï¼‰')
    clean_filename = clean_filename.strip()
    
    # å¦‚æœæ–‡ä»¶åå·²ç»åŒ…å«æ‰©å±•åï¼Œå…ˆæå–åŸºç¡€åç§°
    if '.' in clean_filename:
        clean_filename = Path(clean_filename).stem
    
    # æ”¯æŒçš„æ‰©å±•åå’Œå¯¹åº”çš„æ–‡ä»¶ç±»å‹
    extensions_map = {
        'video': ['.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv'],
        'image': ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'],
        'pdf': ['.pdf'],
        'other': ['.doc', '.docx', '.xls', '.xlsx', '.txt', '.ppt', '.pptx']
    }
    
    # å°è¯•ä¸åŒçš„æ‰©å±•åï¼Œé€’å½’æœç´¢æ‰€æœ‰å­æ–‡ä»¶å¤¹
    for asset_type, extensions in extensions_map.items():
        for ext in extensions:
            # 1. å…ˆå°è¯•ç²¾ç¡®åŒ¹é…ï¼ˆæ–‡ä»¶åå®Œå…¨åŒ¹é…ï¼ŒåŒ…æ‹¬æ‰©å±•åï¼‰
            full_filename = clean_filename + ext
            
            # é€’å½’æœç´¢ï¼šä»æ ¹ç›®å½•å¼€å§‹ï¼Œéå†æ‰€æœ‰å­æ–‡ä»¶å¤¹
            for file_path in base_path.rglob(full_filename):
                if file_path.is_file():
                    file_size = file_path.stat().st_size
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„ç”¨äºç”Ÿæˆ URL
                    relative_path = file_path.relative_to(base_path)
                    # URL æ ¼å¼ï¼š{ATTACHMENT_BASE_URL}/{ç›¸å¯¹è·¯å¾„}ï¼Œå¯¹è·¯å¾„è¿›è¡ŒURLç¼–ç ä»¥æ”¯æŒä¸­æ–‡
                    relative_path_str = str(relative_path).replace(os.sep, '/')
                    # å¯¹è·¯å¾„çš„æ¯ä¸€éƒ¨åˆ†è¿›è¡Œç¼–ç ï¼Œä½†ä¿ç•™æ–œæ 
                    encoded_path = '/'.join(quote(part, safe='') for part in relative_path_str.split('/'))
                    file_url = f"{ATTACHMENT_BASE_URL.rstrip('/')}/{encoded_path}"
                    logger.info(f"æ‰¾åˆ°é™„ä»¶æ–‡ä»¶ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰: {clean_filename} -> {file_path.name} (è·¯å¾„: {relative_path})")
                    return {
                        "path": str(file_path),
                        "url": file_url,
                        "type": asset_type,
                        "size": file_size,
                        "file_name": file_path.name
                    }
            
            # 2. å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆæ–‡ä»¶ååŒ…å«å…³é”®å­—ï¼‰
            # ä¾‹å¦‚ï¼šfilename="ä¸‹èŠ¯æœºæ¯”ä¾‹é˜€æ‹†è§£"ï¼Œå¯èƒ½åŒ¹é… "ä¸‹èŠ¯æœºæ¯”ä¾‹é˜€æ‹†è§£.mp4" æˆ– "ä¸‹èŠ¯æœºæ¯”ä¾‹é˜€æ‹†è£….mp4"
            pattern = f"*{clean_filename}*{ext}"
            matches = list(base_path.rglob(pattern))
            if matches:
                # ä¼˜å…ˆé€‰æ‹©æ–‡ä»¶åæœ€æ¥è¿‘çš„ï¼ˆåŒ…å«å®Œæ•´å…³é”®å­—ï¼‰
                best_match = None
                for file_path in matches:
                    if file_path.is_file():
                        file_name_lower = file_path.stem.lower()
                        clean_lower = clean_filename.lower()
                        # å¦‚æœæ–‡ä»¶ååŒ…å«å®Œæ•´çš„å…³é”®å­—ï¼Œä¼˜å…ˆé€‰æ‹©
                        if clean_lower in file_name_lower:
                            best_match = file_path
                            break
                
                # å¦‚æœæ²¡æœ‰å®Œå…¨åŒ¹é…çš„ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
                if not best_match and matches:
                    for file_path in matches:
                        if file_path.is_file():
                            best_match = file_path
                            break
                
                if best_match and best_match.is_file():
                    file_size = best_match.stat().st_size
                    relative_path = best_match.relative_to(base_path)
                    relative_path_str = str(relative_path).replace(os.sep, '/')
                    encoded_path = '/'.join(quote(part, safe='') for part in relative_path_str.split('/'))
                    file_url = f"{ATTACHMENT_BASE_URL.rstrip('/')}/{encoded_path}"
                    logger.info(f"æ‰¾åˆ°é™„ä»¶æ–‡ä»¶ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰: {clean_filename} -> {best_match.name} (è·¯å¾„: {relative_path})")
                    return {
                        "path": str(best_match),
                        "url": file_url,
                        "type": asset_type,
                        "size": file_size,
                        "file_name": best_match.name
                    }
    
    # 3. å¦‚æœæ–‡ä»¶åŒ¹é…å¤±è´¥ï¼Œå°è¯•åŒ¹é…æ–‡ä»¶å¤¹åç§°
    # ä¾‹å¦‚ï¼šExcel å¼•ç”¨ "åŠ ç ‚çƒé˜€æ¸…æŒ¤ç ‚"ï¼ŒåŒ¹é…æ–‡ä»¶å¤¹ "åŠ ç ‚çƒé˜€æ¸…æŒ¤ç ‚"ï¼Œè¿”å›æ–‡ä»¶å¤¹å†…çš„ç¬¬ä¸€ä¸ªæ–‡ä»¶
    logger.debug(f"æ–‡ä»¶åŒ¹é…å¤±è´¥ï¼Œå°è¯•åŒ¹é…æ–‡ä»¶å¤¹åç§°: {clean_filename}")
    clean_lower = clean_filename.lower()
    
    # æ”¶é›†æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶å¤¹ï¼ˆä¼˜å…ˆå®Œå…¨åŒ¹é…ï¼‰
    matched_folders = []
    for folder_path in base_path.rglob("*"):
        if folder_path.is_dir():
            folder_name = folder_path.name
            folder_lower = folder_name.lower()
            
            # å®Œå…¨åŒ¹é…ä¼˜å…ˆ
            if clean_lower == folder_lower:
                matched_folders.insert(0, (folder_path, folder_name, 1))  # ä¼˜å…ˆçº§ 1ï¼šå®Œå…¨åŒ¹é…
            # åŒ…å«åŒ¹é…
            elif clean_lower in folder_lower or folder_lower in clean_lower:
                matched_folders.append((folder_path, folder_name, 2))  # ä¼˜å…ˆçº§ 2ï¼šåŒ…å«åŒ¹é…
    
    # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œä¼˜å…ˆå¤„ç†å®Œå…¨åŒ¹é…çš„æ–‡ä»¶å¤¹
    matched_folders.sort(key=lambda x: x[2])
    
    # åœ¨åŒ¹é…çš„æ–‡ä»¶å¤¹ä¸­æŸ¥æ‰¾æ–‡ä»¶
    for folder_path, folder_name, priority in matched_folders:
        # æŒ‰æ–‡ä»¶ç±»å‹ä¼˜å…ˆçº§æŸ¥æ‰¾ï¼ˆè§†é¢‘ > å›¾ç‰‡ > PDF > å…¶ä»–ï¼‰
        type_priority = ['video', 'image', 'pdf', 'other']
        for asset_type in type_priority:
            if asset_type in extensions_map:
                for ext in extensions_map[asset_type]:
                    for file_path in folder_path.glob(f"*{ext}"):
                        if file_path.is_file():
                            file_size = file_path.stat().st_size
                            relative_path = file_path.relative_to(base_path)
                            relative_path_str = str(relative_path).replace(os.sep, '/')
                            encoded_path = '/'.join(quote(part, safe='') for part in relative_path_str.split('/'))
                            file_url = f"{ATTACHMENT_BASE_URL.rstrip('/')}/{encoded_path}"
                            match_type = "å®Œå…¨åŒ¹é…" if priority == 1 else "åŒ…å«åŒ¹é…"
                            logger.info(f"æ‰¾åˆ°é™„ä»¶æ–‡ä»¶ï¼ˆæ–‡ä»¶å¤¹{match_type}ï¼‰: {clean_filename} -> æ–‡ä»¶å¤¹[{folder_name}]/{file_path.name} (è·¯å¾„: {relative_path})")
                            return {
                                "path": str(file_path),
                                "url": file_url,
                                "type": asset_type,
                                "size": file_size,
                                "file_name": file_path.name
                            }
    
    logger.debug(f"æœªæ‰¾åˆ°é™„ä»¶æ–‡ä»¶: {filename} (æ¸…ç†å: {clean_filename}, æœç´¢è·¯å¾„: {base_path})")
    return None


def map_excel_row_to_article(row: pd.Series, source_file_name: str, sheet_name: str, row_index: int) -> Optional[dict]:
    """
    å°† Excel è¡Œæ˜ å°„ä¸ºçŸ¥è¯†æ¡ç›® DTOï¼ˆå¿ å®è¿˜åŸåŸæ–‡æ¡£æ¨¡å¼ï¼‰
    
    è¡¨å¤´ï¼šåºå· | ç°è±¡ï¼ˆé—®é¢˜ï¼‰ | æ£€æŸ¥ç‚¹ï¼ˆåŸå› ï¼‰ | ç»´ä¿®å¯¹ç­–ï¼ˆè§£å†³åŠæ³•ï¼‰ | ç»´ä¿®è§†é¢‘ï¼ˆé™„ä»¶ï¼‰
    """
    # è¯»å–å­—æ®µï¼ˆåŸæ ·ä¿ç•™ï¼Œä¸ä¿®æ”¹ï¼‰
    # æ³¨æ„ï¼špandas è¯»å–æ—¶ï¼Œåˆ—åå¯èƒ½åŒ…å«å‰åç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦
    # æ”¯æŒå¤šç§åˆ—åæ ¼å¼ï¼ˆå¸¦æ‹¬å·å’Œä¸å¸¦æ‹¬å·ï¼‰
    serial_number = ""
    for col_name in ["åºå·"]:
        if col_name in row.index and pd.notna(row.get(col_name)):
            serial_number = str(row.get(col_name)).strip()
            break
    
    phenomenon = ""
    # å°è¯•å¤šç§å¯èƒ½çš„åˆ—åï¼ˆåŒ…æ‹¬å¸¦æ‹¬å·å’Œä¸å¸¦æ‹¬å·çš„å˜ä½“ï¼‰
    for col_name in ["ç°è±¡ï¼ˆé—®é¢˜ï¼‰", "ç°è±¡(é—®é¢˜)", "ç°è±¡ ï¼ˆé—®é¢˜ï¼‰", "ç°è±¡ (é—®é¢˜)", "ç°è±¡", "é—®é¢˜"]:
        if col_name in row.index and pd.notna(row.get(col_name)):
            val = row.get(col_name)
            if val is not None and str(val).strip():
                phenomenon = str(val).strip()
                break
    
    checkpoints = ""
    for col_name in ["æ£€æŸ¥ç‚¹ï¼ˆåŸå› ï¼‰", "æ£€æŸ¥ç‚¹(åŸå› )", "æ£€æŸ¥ç‚¹ ï¼ˆåŸå› ï¼‰", "æ£€æŸ¥ç‚¹ (åŸå› )", "æ£€æŸ¥ç‚¹", "åŸå› "]:
        if col_name in row.index and pd.notna(row.get(col_name)):
            val = row.get(col_name)
            if val is not None and str(val).strip():
                checkpoints = str(val).strip()
                break
    
    solution = ""
    for col_name in ["ç»´ä¿®å¯¹ç­–ï¼ˆè§£å†³åŠæ³•ï¼‰", "ç»´ä¿®å¯¹ç­–(è§£å†³åŠæ³•)", "ç»´ä¿®å¯¹ç­– ï¼ˆè§£å†³åŠæ³•ï¼‰", "ç»´ä¿®å¯¹ç­– (è§£å†³åŠæ³•)", "å¯¹ç­–", "ç»´ä¿®å¯¹ç­–", "è§£å†³åŠæ³•", "è§£å†³æ–¹æ³•"]:
        if col_name in row.index and pd.notna(row.get(col_name)):
            val = row.get(col_name)
            if val is not None and str(val).strip():
                solution = str(val).strip()
                break
    
    video_reference = ""
    for col_name in ["ç»´ä¿®è§†é¢‘ï¼ˆé™„ä»¶ï¼‰", "ç»´ä¿®è§†é¢‘(é™„ä»¶)", "ç»´ä¿®è§†é¢‘ ï¼ˆé™„ä»¶ï¼‰", "ç»´ä¿®è§†é¢‘ (é™„ä»¶)", "ç»´ä¿®è§†é¢‘", "é™„ä»¶", "å¤‡æ³¨"]:
        if col_name in row.index and pd.notna(row.get(col_name)):
            val = row.get(col_name)
            if val is not None and str(val).strip():
                video_reference = str(val).strip()
                break
    
    # è·³è¿‡ç©ºè¡Œï¼ˆå¿…é¡»æœ‰ç°è±¡ï¼ˆé—®é¢˜ï¼‰ï¼‰
    if not phenomenon:
        return None
    
    # 1) title = ã€YH400/YH500ã€‘ + ç°è±¡ï¼ˆé—®é¢˜ï¼‰
    title = f"ã€YH400/YH500ã€‘{phenomenon}".strip()
    if not title:
        return None
    
    # 2) question_text å¿…é¡»ä¿ç•™åŸå­—æ®µåä¸åŸæ–‡
    question_parts = []
    if serial_number:
        question_parts.append(f"ã€åºå·ã€‘{serial_number}")
    if phenomenon:
        question_parts.append(f"ã€ç°è±¡ï¼ˆé—®é¢˜ï¼‰ã€‘{phenomenon}")
    question_text = "\n".join(question_parts) if question_parts else None
    
    # 3) cause_text = åŸæ ·å†™å…¥ æ£€æŸ¥ç‚¹ï¼ˆåŸå› ï¼‰ï¼ˆä¿ç•™ç¼–å·/æ¢è¡Œï¼‰
    cause_text = None
    if checkpoints:
        cause_text = f"ã€æ£€æŸ¥ç‚¹ï¼ˆåŸå› ï¼‰ã€‘\n{checkpoints}"
    
    # 4) solution_text = åŸæ ·å†™å…¥ ç»´ä¿®å¯¹ç­–ï¼ˆè§£å†³åŠæ³•ï¼‰ï¼ˆä¿ç•™ç¼–å·/æ¢è¡Œï¼‰
    solution_parts = []
    if solution:
        solution_parts.append(f"ã€ç»´ä¿®å¯¹ç­–ï¼ˆè§£å†³åŠæ³•ï¼‰ã€‘\n{solution}")
    
    # 5) ç»´ä¿®è§†é¢‘ï¼ˆé™„ä»¶ï¼‰ï¼šå¦‚æœæ˜¯æ–‡æœ¬ï¼Œè¿½åŠ åˆ° solution_text
    if video_reference:
        solution_parts.append(f"ã€ç»´ä¿®è§†é¢‘ï¼ˆé™„ä»¶ï¼‰ã€‘\n{video_reference}")
    
    solution_text = "\n\n".join(solution_parts) if solution_parts else None
    
    # 6) scope_json å¿…é¡»åŒ…å«ï¼šè®¾å¤‡ç³»åˆ—ã€æ¥æºæ–‡ä»¶ã€sheetã€è¡Œå·
    scope_data = {
        "è®¾å¤‡ç³»åˆ—": "YH400/YH500",
        "æ¥æºæ–‡ä»¶": source_file_name,
        "sheet": sheet_name,
        "è¡Œå·": row_index
    }
    scope_json = json.dumps(scope_data, ensure_ascii=False)
    
    # 7) tags è‡³å°‘åŒ…å«ï¼šYH400, YH500, æ¥æº:{æ–‡ä»¶å}
    tags = ["YH400", "YH500"]
    file_name_without_ext = Path(source_file_name).stem
    if file_name_without_ext:
        tags.append(f"æ¥æº:{file_name_without_ext}")
    
    # å¦‚æœç°è±¡/åŸå› ä¸­åŒ…å«å…³é”®å­—ä¹Ÿå¯è¿½åŠ ï¼ˆç®€å•å…³é”®è¯æå–ï¼‰
    text_content = f"{phenomenon} {checkpoints} {solution}".lower()
    if "yh400" in text_content and "YH400" not in tags:
        # å·²ç»åœ¨tagsä¸­ï¼Œè·³è¿‡
        pass
    if "yh500" in text_content and "YH500" not in tags:
        # å·²ç»åœ¨tagsä¸­ï¼Œè·³è¿‡
        pass
    
    # æå–é™„ä»¶ä¿¡æ¯ï¼ˆç”¨äºåç»­åˆ›å»º kb_assetï¼‰
    # æ”¯æŒä¸€ä¸ªå•å…ƒæ ¼åŒ…å«å¤šä¸ªå¼•ç”¨ï¼ˆç”¨æ¢è¡Œç¬¦ã€åˆ†å·ç­‰åˆ†éš”ï¼‰ï¼Œæ¯ä¸ªå¼•ç”¨éƒ½å°è¯•åŒ¹é…å¹¶åˆ›å»ºé™„ä»¶è®°å½•
    attachment_info_list = []  # æ”¹ä¸ºåˆ—è¡¨ï¼Œæ”¯æŒå¤šä¸ªé™„ä»¶
    has_attachment_reference = False  # æ ‡è®°æ˜¯å¦æœ‰é™„ä»¶å¼•ç”¨ï¼ˆæ— è®ºæ˜¯å¦æ‰¾åˆ°æ–‡ä»¶ï¼‰
    if video_reference:
        has_attachment_reference = True
        # å…ˆæŒ‰æ¢è¡Œç¬¦åˆ†å‰²ï¼Œç„¶åå¯¹æ¯ä¸€è¡Œå†æŒ‰åˆ†å·ã€ä¸­æ–‡åˆ†å·ç­‰åˆ†å‰²
        # è¿™æ ·å¯ä»¥å¤„ç†å¤šç§æ ¼å¼ï¼šæ¢è¡Œåˆ†éš”ã€åˆ†å·åˆ†éš”ã€æˆ–æ··åˆ
        all_references = []
        for line in video_reference.split('\n'):
            line = line.strip()
            if not line:
                continue
            # æŒ‰åˆ†å·ã€ä¸­æ–‡åˆ†å·åˆ†å‰²
            parts = re.split(r'[ï¼›;]', line)
            for part in parts:
                part = part.strip()
                if part:
                    all_references.append(part)
        
        # å¦‚æœæŒ‰åˆ†éš”ç¬¦åˆ†å‰²ååªæœ‰ä¸€ä¸ªï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰"å‚è€ƒ"xxx""æ ¼å¼çš„å¼•ç”¨
        if len(all_references) == 1:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰"å‚è€ƒ"xxx""æ ¼å¼çš„å¼•ç”¨
            pattern = r'å‚è€ƒ["""""]([^"""""]+)["""""]'
            matches = re.findall(pattern, video_reference)
            if matches:
                all_references = [f'å‚è€ƒ"{m}"' for m in matches]
        
        if len(all_references) > 1:
            logger.info(f"æ£€æµ‹åˆ° {len(all_references)} ä¸ªå¼•ç”¨ï¼Œå°†å°è¯•åŒ¹é…æ‰€æœ‰å¼•ç”¨")
        else:
            logger.debug(f"æå–åˆ° {len(all_references)} ä¸ªå¼•ç”¨: {all_references}")
        
        for ref_line in all_references:
            extracted_filename = extract_filename_from_reference(ref_line)
            if extracted_filename:
                # å†æ¬¡æ¸…ç†æ–‡ä»¶åï¼ˆç¡®ä¿å»é™¤æ‰€æœ‰å¼•å·ï¼‰
                clean_extracted = extracted_filename.strip('"""\'"ã€Šã€‹ã€ã€‘[]()ï¼ˆï¼‰').strip()
                if not clean_extracted:
                    continue
                
                file_info = find_attachment_file(clean_extracted)
                if file_info:
                    # æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶ï¼Œæ·»åŠ åˆ°åˆ—è¡¨ï¼ˆä¸å† breakï¼Œç»§ç»­å¤„ç†å…¶ä»–å¼•ç”¨ï¼‰
                    attachment_info_list.append({
                        "filename": clean_extracted,
                        "file_name": file_info.get("file_name", os.path.basename(file_info["path"])),
                        "url": file_info["url"],
                        "asset_type": file_info["type"],
                        "size": file_info["size"]
                    })
                    logger.info(f"âœ“ æ‰¾åˆ°é™„ä»¶æ–‡ä»¶: {clean_extracted} -> {file_info['url']}")
                else:
                    logger.debug(f"æœªæ‰¾åˆ°é™„ä»¶æ–‡ä»¶: {clean_extracted} (åŸå§‹å¼•ç”¨: {ref_line[:50]}...)")
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªé™„ä»¶ï¼Œä¿æŒå‘åå…¼å®¹ï¼ˆå•ä¸ªå¯¹è±¡ï¼‰ï¼Œå¦‚æœæœ‰å¤šä¸ªï¼Œä½¿ç”¨åˆ—è¡¨
    attachment_info = attachment_info_list[0] if len(attachment_info_list) == 1 else (attachment_info_list if attachment_info_list else None)
    
    return {
        "title": title,
        "questionText": question_text,
        "causeText": cause_text,
        "solutionText": solution_text,
        "scopeJson": scope_json,
        "tags": ", ".join(tags),
        "createdBy": "ç³»ç»Ÿå¯¼å…¥",
        "_attachment_info": attachment_info  # å†…éƒ¨å­—æ®µï¼Œç”¨äºåç»­åˆ›å»ºé™„ä»¶
    }


@app.post("/import/excel", response_model=ExcelImportResponse)
async def import_excel(file: UploadFile = File(...)):
    """
    å¯¼å…¥ Excel æ–‡ä»¶ä¸ºçŸ¥è¯†æ¡ç›®
    
    - æ¥æ”¶ .xlsx æ–‡ä»¶
    - ä½¿ç”¨ pandas è¯»å–
    - æ¯è¡Œæ˜ å°„ä¸ºä¸€æ¡çŸ¥è¯†è‰ç¨¿
    - è°ƒç”¨ .NET çš„ /api/ai/kb/articles/batch åˆ›å»ºè‰ç¨¿
    - å¦‚æœæ‰¾åˆ°é™„ä»¶æ–‡ä»¶ï¼Œåˆ›å»º kb_asset è®°å½•ï¼ˆæ–¹æ¡ˆ Bï¼šå…ƒæ•°æ®å…³è”ï¼‰
    """
    logger.info(f"æ”¶åˆ° Excel å¯¼å…¥è¯·æ±‚: {file.filename}")
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    if not file.filename or not file.filename.endswith('.xlsx'):
        logger.warning(f"æ–‡ä»¶ç±»å‹ä¸æ­£ç¡®: {file.filename}")
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ .xlsx æ ¼å¼çš„ Excel æ–‡ä»¶")
    
    try:
        # è¯»å– Excel æ–‡ä»¶
        contents = await file.read()
        # å°† bytes è½¬æ¢ä¸º BytesIO å¯¹è±¡ï¼Œpandas æ‰èƒ½è¯»å–
        excel_file = BytesIO(contents)
        
        # å…ˆå°è¯•ä»ç¬¬ä¸€è¡Œè¯»å–è¡¨å¤´
        df = pd.read_excel(excel_file, engine='openpyxl', header=0)
        
        # éªŒè¯å¿…éœ€å­—æ®µï¼ˆæ”¯æŒå¤šç§åˆ—åï¼‰
        # æ•…éšœç°è±¡åˆ—åå¯èƒ½ä¸ºï¼šæ•…éšœç°è±¡ã€ç°è±¡ï¼ˆé—®é¢˜ï¼‰ã€ç°è±¡ã€é—®é¢˜
        fault_phenomenon_columns = ["æ•…éšœç°è±¡", "ç°è±¡ï¼ˆé—®é¢˜ï¼‰", "ç°è±¡(é—®é¢˜)", "ç°è±¡", "é—®é¢˜"]
        has_fault_phenomenon = any(col in df.columns for col in fault_phenomenon_columns)
        header_row = 0  # è®°å½• header è¡Œå·
        
        # å¦‚æœç¬¬ä¸€è¡Œæ²¡æœ‰æ‰¾åˆ°å¿…éœ€å­—æ®µï¼Œå°è¯•ä»ç¬¬äºŒè¡Œè¯»å–ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰
        if not has_fault_phenomenon:
            logger.info("ç¬¬ä¸€è¡Œæœªæ‰¾åˆ°å¿…éœ€å­—æ®µï¼Œå°è¯•ä»ç¬¬äºŒè¡Œè¯»å–è¡¨å¤´ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰")
            excel_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            df = pd.read_excel(excel_file, engine='openpyxl', header=1)
            header_row = 1  # æ›´æ–° header è¡Œå·
            
            # å†æ¬¡éªŒè¯
            has_fault_phenomenon = any(col in df.columns for col in fault_phenomenon_columns)
            if not has_fault_phenomenon:
                raise HTTPException(
                    status_code=400,
                    detail=f"ç¼ºå°‘å¿…éœ€å­—æ®µ: è¯·åŒ…å«ä»¥ä¸‹ä»»ä¸€åˆ—å - {', '.join(fault_phenomenon_columns)}"
                )
        
        if df.empty:
            raise HTTPException(status_code=400, detail="Excel æ–‡ä»¶ä¸ºç©º")
        
        logger.info(f"è¯»å–åˆ° {len(df)} è¡Œæ•°æ®")
        logger.info(f"åˆ—å: {list(df.columns)}")
        
        # è·å– sheet åç§°ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        sheet_name = "Sheet1"  # é»˜è®¤å€¼
        try:
            # å°è¯•ä» Excel æ–‡ä»¶ä¸­è·å– sheet åç§°
            excel_file.seek(0)
            import openpyxl
            wb = openpyxl.load_workbook(excel_file, read_only=True)
            if wb.sheetnames:
                sheet_name = wb.sheetnames[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ª sheet çš„åç§°
            excel_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        except Exception as e:
            logger.warning(f"æ— æ³•è·å– sheet åç§°ï¼Œä½¿ç”¨é»˜è®¤å€¼: {str(e)}")
        
        # ç¡®å®š header è¡Œå·ï¼ˆç”¨äºè®¡ç®— Excel è¡Œå·ï¼‰
        header_row = 0 if has_fault_phenomenon else 1
        
        # æ˜ å°„æ¯è¡Œä¸ºçŸ¥è¯†æ¡ç›® DTO
        articles = []  # å‘é€ç»™åç«¯çš„æ–‡ç« ï¼ˆä¸åŒ…å«å†…éƒ¨å­—æ®µï¼‰
        articles_with_attachments = []  # åŒ…å«é™„ä»¶ä¿¡æ¯çš„å®Œæ•´æ–‡ç« æ•°æ®
        failures = []
        skipped_rows = 0
        attachment_match_count = 0  # é™„ä»¶åŒ¹é…ç»Ÿè®¡
        attachment_not_found_count = 0  # é™„ä»¶æœªæ‰¾åˆ°ç»Ÿè®¡
        
        for idx, row in df.iterrows():
            try:
                # Excel è¡Œå· = pandas index + headerè¡Œæ•° + 1ï¼ˆExcelä»1å¼€å§‹è®¡æ•°ï¼‰
                # å¦‚æœ header=0ï¼Œåˆ™ Excelè¡Œå· = idx + 2ï¼ˆç¬¬1è¡Œæ˜¯è¡¨å¤´ï¼Œç¬¬2è¡Œå¼€å§‹æ˜¯æ•°æ®ï¼‰
                # å¦‚æœ header=1ï¼Œåˆ™ Excelè¡Œå· = idx + 3ï¼ˆç¬¬1è¡Œæ˜¯æ ‡é¢˜ï¼Œç¬¬2è¡Œæ˜¯è¡¨å¤´ï¼Œç¬¬3è¡Œå¼€å§‹æ˜¯æ•°æ®ï¼‰
                excel_row_number = int(idx) + header_row + 2
                
                # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€è¡Œçš„æ•°æ®
                if idx == 0:
                    logger.info(f"ç¬¬ä¸€è¡Œæ•°æ®é¢„è§ˆ: {dict(row)}")
                
                article = map_excel_row_to_article(row, file.filename, sheet_name, excel_row_number)
                if article:
                    # ä¿å­˜åŒ…å«é™„ä»¶ä¿¡æ¯çš„åŸå§‹ article
                    articles_with_attachments.append(article)
                    # ç§»é™¤å†…éƒ¨å­—æ®µï¼ˆä¸å‘é€ç»™åç«¯ï¼‰
                    article_for_api = {k: v for k, v in article.items() if not k.startswith("_")}
                    articles.append(article_for_api)
                else:
                    skipped_rows += 1
                    if idx < 3:  # åªè®°å½•å‰3è¡Œçš„è·³è¿‡åŸå› 
                        logger.info(f"ç¬¬ {excel_row_number} è¡Œè¢«è·³è¿‡ï¼ˆç©ºè¡Œæˆ–ç¼ºå°‘å¿…éœ€å­—æ®µï¼‰")
                # å¦‚æœè¿”å› Noneï¼Œè¯´æ˜æ˜¯ç©ºè¡Œï¼Œè·³è¿‡
            except Exception as e:
                excel_row_number = int(idx) + header_row + 2
                logger.error(f"å¤„ç†ç¬¬ {excel_row_number} è¡Œæ—¶å‡ºé”™: {str(e)}")
                failures.append({
                    "row_index": excel_row_number,
                    "reason": str(e)
                })
        
        # ç»Ÿè®¡é™„ä»¶åŒ¹é…æƒ…å†µï¼ˆæ”¯æŒå¤šä¸ªé™„ä»¶ï¼‰
        attachment_match_count = 0
        for a in articles_with_attachments:
            att_info = a.get("_attachment_info")
            if att_info:
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç»Ÿè®¡åˆ—è¡¨é•¿åº¦ï¼›å¦‚æœæ˜¯å•ä¸ªå¯¹è±¡ï¼Œè®¡æ•°ä¸º1
                if isinstance(att_info, list):
                    attachment_match_count += len(att_info)
                else:
                    attachment_match_count += 1
        
        attachment_total = sum(1 for a in articles_with_attachments if a.get("_has_attachment_reference", False))
        attachment_not_found_count = attachment_total - sum(1 for a in articles_with_attachments if a.get("_attachment_info"))
        
        logger.info(f"æˆåŠŸæ˜ å°„ {len(articles)} æ¡ï¼Œè·³è¿‡ {skipped_rows} è¡Œï¼Œå¤±è´¥ {len(failures)} è¡Œ")
        if attachment_total > 0:
            match_rate = (attachment_match_count / attachment_total * 100) if attachment_total > 0 else 0
            logger.info(f"ğŸ“ é™„ä»¶åŒ¹é…ç»Ÿè®¡: æ‰¾åˆ° {attachment_match_count}/{attachment_total} ä¸ª ({match_rate:.1f}%)ï¼Œæœªæ‰¾åˆ° {attachment_not_found_count} ä¸ª")
        
        if not articles:
            error_msg = f"æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®è¡Œã€‚æ€»è¡Œæ•°: {len(df)}, è·³è¿‡: {skipped_rows}, å¤±è´¥: {len(failures)}"
            if len(df) > 0:
                error_msg += f"\nåˆ—å: {list(df.columns)}"
                error_msg += f"\nç¬¬ä¸€è¡Œæ•°æ®: {dict(df.iloc[0]) if len(df) > 0 else 'N/A'}"
            raise HTTPException(status_code=400, detail=error_msg)
        
        # è°ƒç”¨ .NET åç«¯æ‰¹é‡åˆ›å»ºæ¥å£
        logger.info(f"å‡†å¤‡è°ƒç”¨ .NET åç«¯: {DOTNET_BASE_URL}/api/ai/kb/articles/batch, æ–‡ç« æ•°é‡: {len(articles)}")
        
        async with httpx.AsyncClient() as client:
            try:
                response = await client.post(
                    f"{DOTNET_BASE_URL}/api/ai/kb/articles/batch",
                    json={"articles": articles},
                    headers={
                        "Content-Type": "application/json",
                        "X-Tenant-Id": DEFAULT_TENANT,
                        "X-Internal-Token": INTERNAL_TOKEN
                    },
                    timeout=30.0
                )
                
                logger.info(f".NET åç«¯å“åº”çŠ¶æ€ç : {response.status_code}")
                
                if response.status_code != 200:
                    error_text = response.text
                    logger.error(f".NET åç«¯è¿”å›é”™è¯¯: {error_text}")
                    raise HTTPException(
                        status_code=response.status_code,
                        detail=f".NET åç«¯è¿”å›é”™è¯¯: {error_text}"
                    )
            except httpx.ConnectError as e:
                logger.error(f"æ— æ³•è¿æ¥åˆ° .NET åç«¯ {DOTNET_BASE_URL}: {str(e)}")
                raise HTTPException(
                    status_code=503,
                    detail=f"æ— æ³•è¿æ¥åˆ° .NET åç«¯æœåŠ¡ï¼Œè¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨: {DOTNET_BASE_URL}"
                )
            except httpx.TimeoutException as e:
                logger.error(f"è°ƒç”¨ .NET åç«¯è¶…æ—¶: {str(e)}")
                raise HTTPException(
                    status_code=504,
                    detail="è°ƒç”¨ .NET åç«¯è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
                )
            
            result = response.json()
            
            # å¤„ç†è¿”å›ç»“æœ
            success_count = result.get("successCount", 0)
            failure_count = result.get("failureCount", 0)
            article_ids = []
            article_id_to_attachment = {}  # æ˜ å°„ article_id -> attachment_info
            
            # æ”¶é›†æˆåŠŸåˆ›å»ºçš„ article IDs å’Œé™„ä»¶ä¿¡æ¯
            # æ”¯æŒä¸€ä¸ªæ–‡ç« æœ‰å¤šä¸ªé™„ä»¶ï¼ˆ_attachment_info å¯èƒ½æ˜¯å•ä¸ªå¯¹è±¡æˆ–åˆ—è¡¨ï¼‰
            article_id_to_attachments = {}  # æ”¹ä¸ºæ˜ å°„åˆ°é™„ä»¶åˆ—è¡¨
            for i, item in enumerate(result.get("results", [])):
                if item.get("success") and item.get("articleId"):
                    article_id = item["articleId"]
                    article_ids.append(article_id)
                    
                    # å¦‚æœå¯¹åº”çš„ article æœ‰é™„ä»¶ä¿¡æ¯ï¼Œä¿å­˜æ˜ å°„
                    if i < len(articles_with_attachments):
                        att_info = articles_with_attachments[i].get("_attachment_info")
                        if att_info:
                            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦‚æœæ˜¯å•ä¸ªå¯¹è±¡ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                            if isinstance(att_info, list):
                                article_id_to_attachments[article_id] = att_info
                            else:
                                article_id_to_attachments[article_id] = [att_info]
                elif not item.get("success"):
                    # è®°å½•åç«¯è¿”å›çš„å¤±è´¥ä¿¡æ¯
                    failures.append({
                        "row_index": item.get("index", -1) + (header_row + 2),  # è½¬æ¢ä¸º Excel è¡Œå·
                        "reason": item.get("error", "æœªçŸ¥é”™è¯¯")
                    })
            
            # æ‰¹é‡åˆ›å»ºé™„ä»¶ï¼ˆæ–¹æ¡ˆ Bï¼šå…ƒæ•°æ®å…³è”ï¼‰
            if article_id_to_attachments:
                total_attachments = sum(len(atts) for atts in article_id_to_attachments.values())
                logger.info(f"å‡†å¤‡åˆ›å»º {total_attachments} ä¸ªé™„ä»¶è®°å½•ï¼ˆæ¶‰åŠ {len(article_id_to_attachments)} ç¯‡æ–‡ç« ï¼‰")
                
                # è®°å½•æ¯ä¸ªæ–‡ç« çš„é™„ä»¶æ•°é‡
                for article_id, att_info_list in article_id_to_attachments.items():
                    logger.info(f"  æ–‡ç«  ID {article_id}: {len(att_info_list)} ä¸ªé™„ä»¶")
                
                assets_to_create = []
                
                for article_id, att_info_list in article_id_to_attachments.items():
                    # ä¸ºæ¯ä¸ªé™„ä»¶åˆ›å»ºè®°å½•
                    for att_info in att_info_list:
                        assets_to_create.append({
                            "articleId": article_id,
                            "assetType": att_info["asset_type"],
                            "fileName": att_info["file_name"],
                            "url": att_info["url"],
                            "size": att_info["size"],
                            "duration": None  # è§†é¢‘æ—¶é•¿æš‚ä¸æ”¯æŒè‡ªåŠ¨è·å–
                        })
                
                if assets_to_create:
                    try:
                        asset_response = await client.post(
                            f"{DOTNET_BASE_URL}/api/ai/kb/articles/assets/batch",
                            json={"assets": assets_to_create},
                            headers={
                                "Content-Type": "application/json",
                                "X-Tenant-Id": DEFAULT_TENANT,
                                "X-Internal-Token": INTERNAL_TOKEN
                            },
                            timeout=30.0
                        )
                        
                        if asset_response.status_code == 200:
                            asset_result = asset_response.json()
                            logger.info(f"æˆåŠŸåˆ›å»º {asset_result.get('successCount', 0)} ä¸ªé™„ä»¶è®°å½•")
                        else:
                            logger.warning(f"åˆ›å»ºé™„ä»¶å¤±è´¥: {asset_response.status_code} - {asset_response.text}")
                    except Exception as e:
                        logger.error(f"åˆ›å»ºé™„ä»¶æ—¶å‡ºé”™: {str(e)}")
                        # é™„ä»¶åˆ›å»ºå¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            
            # åˆå¹¶å‰ç«¯è§£æå¤±è´¥å’Œåç«¯åˆ›å»ºå¤±è´¥
            total_failures = len(failures) + failure_count
            
            return ExcelImportResponse(
                total_rows=len(df),
                success_count=success_count,
                failure_count=total_failures,
                article_ids=article_ids,
                failures=failures
            )
    
    except pd.errors.EmptyDataError:
        logger.error("Excel æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
        raise HTTPException(status_code=400, detail="Excel æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
    except httpx.HTTPError as e:
        logger.error(f"è°ƒç”¨ .NET åç«¯å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=500,
            detail=f"è°ƒç”¨ .NET åç«¯å¤±è´¥: {str(e)}"
        )
    except Exception as e:
        logger.error(f"å¯¼å…¥å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=500,
            detail=f"å¯¼å…¥å¤±è´¥: {str(e)}"
        )


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "service": "ai-hub-ai"}


if __name__ == "__main__":
    import uvicorn
    import os
    port = int(os.getenv("PORT", "8000"))
    uvicorn.run(app, host="0.0.0.0", port=port)
