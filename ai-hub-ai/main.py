"""
AI Hub Excel å¯¼å…¥æœåŠ¡
FastAPI æœåŠ¡ï¼Œç”¨äºå¤„ç† Excel å¯¼å…¥å¹¶è°ƒç”¨ .NET åç«¯ API
"""
import os
import traceback
from typing import List, Optional
from io import BytesIO
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import pandas as pd
import httpx
from pathlib import Path
import json
import re
import logging
from urllib.parse import quote

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="AI Hub Excel å¯¼å…¥æœåŠ¡",
    description="å¤„ç† Excel æ–‡ä»¶å¯¼å…¥ï¼Œè½¬æ¢ä¸ºçŸ¥è¯†æ¡ç›®",
    version="1.0.0"
)

# é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
DOTNET_BASE_URL = os.getenv("DOTNET_BASE_URL", "http://localhost:5000")
INTERNAL_TOKEN = os.getenv("INTERNAL_TOKEN", "your-internal-token-change-in-production")
DEFAULT_TENANT = os.getenv("DEFAULT_TENANT", "default")
ATTACHMENT_BASE_PATH = os.getenv("ATTACHMENT_BASE_PATH", "")
ATTACHMENT_BASE_URL = os.getenv("ATTACHMENT_BASE_URL", "http://localhost:5000/uploads")

# åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from dotenv import load_dotenv
    load_dotenv()
    # é‡æ–°è¯»å–ç¯å¢ƒå˜é‡ï¼ˆ.env æ–‡ä»¶ä¸­çš„å€¼ä¼šè¦†ç›–ä¸Šé¢çš„é»˜è®¤å€¼ï¼‰
    DOTNET_BASE_URL = os.getenv("DOTNET_BASE_URL", DOTNET_BASE_URL)
    INTERNAL_TOKEN = os.getenv("INTERNAL_TOKEN", INTERNAL_TOKEN)
    DEFAULT_TENANT = os.getenv("DEFAULT_TENANT", DEFAULT_TENANT)
    ATTACHMENT_BASE_PATH = os.getenv("ATTACHMENT_BASE_PATH", ATTACHMENT_BASE_PATH)
    ATTACHMENT_BASE_URL = os.getenv("ATTACHMENT_BASE_URL", ATTACHMENT_BASE_URL)
    logger.info(f"é…ç½®åŠ è½½å®Œæˆ: DOTNET_BASE_URL={DOTNET_BASE_URL}, DEFAULT_TENANT={DEFAULT_TENANT}, ATTACHMENT_BASE_PATH={ATTACHMENT_BASE_PATH}")
except ImportError:
    logger.warning("python-dotenv æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼")


class ExcelImportResponse(BaseModel):
    """Excel å¯¼å…¥å“åº”"""
    total_rows: int
    success_count: int
    failure_count: int
    article_ids: List[int]
    failures: List[dict]


class ExcelRowFailure(BaseModel):
    """Excel è¡Œå¤„ç†å¤±è´¥ä¿¡æ¯"""
    row_index: int
    reason: str


def format_as_reasons(text: Optional[str]) -> Optional[str]:
    """æ ¼å¼åŒ–ä¸ºåŸå› åˆ—è¡¨ï¼ˆåŸå›  1ï¼š...ï¼‰"""
    if not text or not text.strip():
        return None
    
    # æŒ‰æ¢è¡Œç¬¦æˆ–åˆ†å·åˆ†å‰²
    reasons = [r.strip() for r in re.split(r'[\n\rï¼›;]', text) if r.strip()]
    
    if not reasons:
        return None
    
    # å¦‚æœå·²ç»æ˜¯"åŸå›  Xï¼š"æ ¼å¼ï¼Œç›´æ¥è¿”å›
    if any(re.match(r'^åŸå› \s*\d+[ï¼š:]', r) for r in reasons):
        return '\n'.join(reasons)
    
    # å¦åˆ™æ·»åŠ ç¼–å·
    return '\n'.join([f"åŸå›  {i + 1}ï¼š{r}" for i, r in enumerate(reasons)])


def format_as_steps(text: Optional[str]) -> Optional[str]:
    """æ ¼å¼åŒ–ä¸ºæ­¥éª¤åˆ—è¡¨ï¼ˆæ­¥éª¤ 1ï¼š...ï¼‰"""
    if not text or not text.strip():
        return None
    
    # æŒ‰æ¢è¡Œç¬¦æˆ–åˆ†å·åˆ†å‰²
    steps = [s.strip() for s in re.split(r'[\n\rï¼›;]', text) if s.strip()]
    
    if not steps:
        return None
    
    # å¦‚æœå·²ç»æ˜¯"æ­¥éª¤ Xï¼š"æ ¼å¼ï¼Œç›´æ¥è¿”å›
    if any(re.match(r'^æ­¥éª¤\s*\d+[ï¼š:]', s) for s in steps):
        return '\n'.join(steps)
    
    # å¦åˆ™æ·»åŠ ç¼–å·
    return '\n'.join([f"æ­¥éª¤ {i + 1}ï¼š{s}" for i, s in enumerate(steps)])


def clean_bracketed_labels(text: str) -> str:
    """
    æ¸…ç†æ–‡æœ¬ä¸­çš„ä¸­æ‹¬å·æ ‡ç­¾å’Œåˆ—æ ‡é¢˜
    ç§»é™¤ï¼šã€åºå·ã€‘ã€ã€ç°è±¡ï¼ˆé—®é¢˜ï¼‰ã€‘ã€ã€æ£€æŸ¥ç‚¹ï¼ˆåŸå› ï¼‰ã€‘ã€ã€ç»´ä¿®å¯¹ç­–ï¼ˆè§£å†³åŠæ³•ï¼‰ã€‘ã€ã€ç»´ä¿®è§†é¢‘ï¼ˆé™„ä»¶ï¼‰ã€‘ã€ã€YH400/YH500ã€‘ç­‰
    
    æ”¯æŒå¤šç§æ ¼å¼ï¼š
    - ã€åºå·ã€‘ã€ã€ç°è±¡ï¼ˆé—®é¢˜ï¼‰ã€‘ç­‰ï¼ˆä¸­æ–‡æ‹¬å·ï¼‰
    - [åºå·]ã€[ç°è±¡(é—®é¢˜)]ç­‰ï¼ˆè‹±æ–‡æ‹¬å·ï¼‰
    - å„ç§å˜ä½“ï¼šç°è±¡(é—®é¢˜)ã€ç°è±¡ï¼ˆé—®é¢˜ï¼‰ã€ç°è±¡ (é—®é¢˜)ç­‰
    """
    if not text or not isinstance(text, str):
        return text if text else ""
    
    import re
    
    # æ›´å…¨é¢çš„æ¨¡å¼åŒ¹é…ï¼Œæ”¯æŒå„ç§å˜ä½“
    # ä½¿ç”¨æ›´ç®€å•ç›´æ¥çš„æ–¹å¼ï¼šåŒ¹é…æ‰€æœ‰åŒ…å«è¿™äº›å…³é”®è¯çš„ä¸­æ‹¬å·å†…å®¹
    patterns_to_remove = [
        # åŒ¹é…ä»»ä½•åŒ…å«"åºå·"çš„ä¸­æ‹¬å·
        r'ã€[^ã€‘]*åºå·[^ã€‘]*ã€‘',
        # åŒ¹é…ä»»ä½•åŒ…å«"ç°è±¡"å’Œ"é—®é¢˜"çš„ä¸­æ‹¬å·ï¼ˆæ”¯æŒå„ç§å˜ä½“ï¼‰
        r'ã€[^ã€‘]*ç°è±¡[^ã€‘]*é—®é¢˜[^ã€‘]*ã€‘',
        r'ã€[^ã€‘]*ç°è±¡[^ã€‘]*ã€‘',
        r'ã€[^ã€‘]*é—®é¢˜[^ã€‘]*ã€‘',
        # åŒ¹é…ä»»ä½•åŒ…å«"æ£€æŸ¥ç‚¹"å’Œ"åŸå› "çš„ä¸­æ‹¬å·
        r'ã€[^ã€‘]*æ£€æŸ¥ç‚¹[^ã€‘]*åŸå› [^ã€‘]*ã€‘',
        r'ã€[^ã€‘]*æ£€æŸ¥ç‚¹[^ã€‘]*ã€‘',
        r'ã€[^ã€‘]*åŸå› [^ã€‘]*ã€‘',
        # åŒ¹é…ä»»ä½•åŒ…å«"ç»´ä¿®å¯¹ç­–"å’Œ"è§£å†³åŠæ³•"çš„ä¸­æ‹¬å·
        r'ã€[^ã€‘]*ç»´ä¿®å¯¹ç­–[^ã€‘]*è§£å†³åŠæ³•[^ã€‘]*ã€‘',
        r'ã€[^ã€‘]*ç»´ä¿®å¯¹ç­–[^ã€‘]*ã€‘',
        r'ã€[^ã€‘]*è§£å†³åŠæ³•[^ã€‘]*ã€‘',
        # åŒ¹é…ä»»ä½•åŒ…å«"ç»´ä¿®è§†é¢‘"å’Œ"é™„ä»¶"çš„ä¸­æ‹¬å·
        r'ã€[^ã€‘]*ç»´ä¿®è§†é¢‘[^ã€‘]*é™„ä»¶[^ã€‘]*ã€‘',
        r'ã€[^ã€‘]*ç»´ä¿®è§†é¢‘[^ã€‘]*ã€‘',
        r'ã€[^ã€‘]*é™„ä»¶[^ã€‘]*ã€‘',
        # åŒ¹é…å‹å·ä¿¡æ¯
        r'ã€[^ã€‘]*YH400[^ã€‘]*YH500[^ã€‘]*ã€‘',
        r'ã€[^ã€‘]*YH400[^ã€‘]*ã€‘',
        r'ã€[^ã€‘]*YH500[^ã€‘]*ã€‘',
        # è‹±æ–‡æ‹¬å·æ ¼å¼ï¼ˆåŒæ ·å¤„ç†ï¼‰
        r'\[[^\]]*åºå·[^\]]*\]',
        r'\[[^\]]*ç°è±¡[^\]]*é—®é¢˜[^\]]*\]',
        r'\[[^\]]*æ£€æŸ¥ç‚¹[^\]]*åŸå› [^\]]*\]',
        r'\[[^\]]*ç»´ä¿®å¯¹ç­–[^\]]*è§£å†³åŠæ³•[^\]]*\]',
        r'\[[^\]]*ç»´ä¿®è§†é¢‘[^\]]*é™„ä»¶[^\]]*\]',
        r'\[[^\]]*YH400[^\]]*YH500[^\]]*\]',
        r'\[[^\]]*YH400[^\]]*\]',
        r'\[[^\]]*YH500[^\]]*\]',
    ]
    
    cleaned_text = text
    # å¤šæ¬¡æ¸…ç†ï¼Œç¡®ä¿æ‰€æœ‰åŒ¹é…éƒ½è¢«ç§»é™¤
    for _ in range(5):  # å¾ªç¯5æ¬¡ï¼Œç¡®ä¿åµŒå¥—æˆ–é‡å¤çš„æ ‡ç­¾éƒ½è¢«æ¸…ç†
        old_text = cleaned_text
        for pattern in patterns_to_remove:
            cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE)
        # å¦‚æœè¿™ä¸€è½®æ²¡æœ‰å˜åŒ–ï¼Œæå‰é€€å‡º
        if cleaned_text == old_text:
            break
    
    # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦ï¼ˆä½†ä¿ç•™æ¢è¡Œï¼‰
    # å…ˆæ¸…ç†è¿ç»­çš„ç©ºæ ¼
    cleaned_text = re.sub(r'[ \t]+', ' ', cleaned_text)
    # æ¸…ç†è¡Œé¦–è¡Œå°¾ç©ºæ ¼ï¼ˆä½†ä¿ç•™æ¢è¡Œï¼‰
    lines = cleaned_text.split('\n')
    cleaned_lines = [line.strip() for line in lines]
    cleaned_text = '\n'.join(cleaned_lines)
    # æ¸…ç†å¤šä½™çš„è¿ç»­æ¢è¡Œï¼ˆæœ€å¤šä¿ç•™ä¸€ä¸ªç©ºè¡Œï¼‰
    cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text)
    cleaned_text = cleaned_text.strip()
    
    return cleaned_text


def extract_filename_from_reference(text: str) -> Optional[str]:
    """
    ä»æ–‡æœ¬å¼•ç”¨ä¸­æå–æ–‡ä»¶å
    æ”¯æŒæ ¼å¼ï¼š
    - å‚è€ƒ"xxx" æˆ– å‚è€ƒ"xxx"ï¼ˆä¸­æ–‡å¼•å· "" å’Œè‹±æ–‡å¼•å· ""ï¼‰
    - å‚è€ƒï¼šxxx
    - è§é™„ä»¶ï¼šxxx
    - å‚è€ƒ xxx
    """
    if not text or not text.strip():
        return None
    
    text = text.strip()
    
    # åŒ¹é…ï¼šå‚è€ƒ"xxx" æˆ– å‚è€ƒ"xxx"ï¼ˆæ”¯æŒä¸­æ–‡å¼•å· "" å’Œè‹±æ–‡å¼•å· ""ï¼‰
    # æ­£åˆ™ï¼šå‚è€ƒ[""]... [""]ï¼ˆæ”¯æŒä¸­æ–‡å¼•å·å’Œè‹±æ–‡å¼•å·ï¼‰
    # ä½¿ç”¨å­—ç¬¦ç±»åŒ¹é…æ‰€æœ‰ç±»å‹çš„å¼•å·
    match = re.search(r'å‚è€ƒ["""""]([^"""""]+)["""""]', text)
    if match:
        filename = match.group(1).strip()
        # å»é™¤å¯èƒ½æ®‹ç•™çš„å¼•å·ï¼ˆåŒ…æ‹¬å…¨è§’å•å¼•å· ''ï¼‰
        filename = filename.strip('"""\'""\'')
        return filename if filename else None
    
    # åŒ¹é…ï¼šå‚è€ƒï¼šxxx æˆ– å‚è€ƒ:xxx
    match = re.search(r'å‚è€ƒ[ï¼š:]\s*(.+)', text)
    if match:
        filename = match.group(1).strip()
        # å»é™¤å¯èƒ½çš„å‰åå¼•å·ï¼ˆåŒ…æ‹¬å…¨è§’å•å¼•å· ''ï¼‰
        filename = filename.strip('"""\'""\'')
        return filename if filename else None
    
    # åŒ¹é…ï¼šè§é™„ä»¶ï¼šxxx
    match = re.search(r'è§é™„ä»¶[ï¼š:]\s*(.+)', text)
    if match:
        filename = match.group(1).strip()
        filename = filename.strip('"""\'""\'')
        return filename if filename else None
    
    # åŒ¹é…ï¼šå‚è€ƒ xxxï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
    match = re.search(r'å‚è€ƒ\s+(.+)', text)
    if match:
        filename = match.group(1).strip()
        filename = filename.strip('"""\'""\'')
        return filename if filename else None
    
    # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œè¿”å›åŸæ–‡ï¼ˆå»é™¤"å‚è€ƒ"ç­‰å‰ç¼€ï¼‰
    filename = re.sub(r'^(å‚è€ƒ|è§é™„ä»¶)[ï¼š:\s]*', '', text)
    filename = filename.strip('"""\'""\'')
    return filename.strip() if filename.strip() else None


def _guess_asset_type_by_ext(ext: str) -> str:
    """æ ¹æ®æ‰©å±•ååˆ¤æ–­æ–‡ä»¶ç±»å‹"""
    ext = ext.lower()
    if ext in ['.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv']:
        return 'video'
    if ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']:
        return 'image'
    if ext in ['.pdf']:
        return 'pdf'
    if ext in ['.doc', '.docx', '.xls', '.xlsx', '.txt', '.ppt', '.pptx']:
        return 'other'
    return 'other'


def _should_skip_file(p: Path) -> bool:
    """è¿‡æ»¤ç³»ç»Ÿ/ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…è„æ•°æ®"""
    name = p.name.lower()
    if name in ['thumbs.db', '.ds_store', 'desktop.ini']:
        return True
    if name.startswith('~$'):  # office ä¸´æ—¶æ–‡ä»¶
        return True
    return False


def _build_file_info(base_path: Path, file_path: Path) -> dict:
    """æ„å»ºæ–‡ä»¶ä¿¡æ¯å­—å…¸"""
    file_size = file_path.stat().st_size
    relative_path = file_path.relative_to(base_path)
    relative_path_str = str(relative_path).replace(os.sep, '/')
    encoded_path = '/'.join(quote(part, safe='') for part in relative_path_str.split('/'))
    file_url = f"{ATTACHMENT_BASE_URL.rstrip('/')}/{encoded_path}"
    return {
        "path": str(file_path),
        "url": file_url,
        "type": _guess_asset_type_by_ext(file_path.suffix),
        "size": file_size,
        "file_name": file_path.name,
        "relative_path": relative_path_str,
    }


def find_attachment_files(filename: str) -> List[dict]:
    """
    åœ¨å›ºå®šç›®å½•ä¸­é€’å½’æŸ¥æ‰¾é™„ä»¶æ–‡ä»¶ï¼ˆæ”¯æŒæ–‡ä»¶å¤¹åµŒå¥—ï¼‰
    è¿”å›ï¼šList[{ "path","url","type","size","file_name", "relative_path" }]
    - å‘½ä¸­æ–‡ä»¶ï¼šè¿”å› 1 æ¡
    - å‘½ä¸­æ–‡ä»¶å¤¹ï¼šè¿”å›è¯¥æ–‡ä»¶å¤¹å†…æ‰€æœ‰æ–‡ä»¶ï¼ˆé€’å½’ï¼‰çš„å¤šæ¡
    - æœªå‘½ä¸­ï¼šè¿”å›ç©ºåˆ—è¡¨
    """
    if not ATTACHMENT_BASE_PATH or not ATTACHMENT_BASE_PATH.strip():
        logger.debug(f"ATTACHMENT_BASE_PATH æœªé…ç½®ï¼Œè·³è¿‡æ–‡ä»¶æŸ¥æ‰¾: {filename}")
        return []

    base_path = Path(ATTACHMENT_BASE_PATH.strip())
    if not base_path.exists():
        logger.debug(f"é™„ä»¶åŸºç¡€è·¯å¾„ä¸å­˜åœ¨: {base_path}ï¼Œè·³è¿‡æ–‡ä»¶æŸ¥æ‰¾: {filename}")
        return []

    # æ¸…ç†æ–‡ä»¶åï¼šå»é™¤å¼•å·ã€æ‹¬å·ç­‰åŒ…è£¹ç¬¦å·ï¼ˆåŒ…æ‹¬å…¨è§’å•å¼•å· ''ï¼‰
    clean_filename = filename.strip().strip('"""\'""\'ã€Šã€‹ã€ã€‘[]()ï¼ˆï¼‰').strip()
    logger.debug(f"å¼€å§‹æŸ¥æ‰¾é™„ä»¶: åŸå§‹æ–‡ä»¶å='{filename}', æ¸…ç†å='{clean_filename}'")

    # å¦‚æœç”¨æˆ·å†™äº†æ‰©å±•åï¼Œå…ˆè½¬ stemï¼ˆExcel é‡Œå¯èƒ½ä¸å¸¦æ‰©å±•åï¼‰
    if '.' in clean_filename:
        original_clean = clean_filename
        clean_filename = Path(clean_filename).stem
        logger.debug(f"æ£€æµ‹åˆ°æ‰©å±•åï¼Œæå–stem: '{original_clean}' -> '{clean_filename}'")

    # æ‰©å±•åæ˜ å°„ï¼ˆç”¨äº"æ–‡ä»¶å‘½ä¸­"é˜¶æ®µï¼‰
    extensions_map = {
        'video': ['.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv'],
        'image': ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'],
        'pdf': ['.pdf'],
        'other': ['.doc', '.docx', '.xls', '.xlsx', '.txt', '.ppt', '.pptx']
    }

    # ========== 1) ç²¾ç¡®åŒ¹é… / é€’å½’ç²¾ç¡®åŒ¹é…ï¼ˆå‘½ä¸­æ–‡ä»¶å°±è¿”å› 1 æ¡ï¼‰ ==========
    for asset_type, extensions in extensions_map.items():
        for ext in extensions:
            full_filename = clean_filename + ext
            logger.debug(f"å°è¯•ç²¾ç¡®åŒ¹é…: '{full_filename}' (ç±»å‹: {asset_type}, æ‰©å±•å: {ext})")

            # æ ¹ç›®å½•ç²¾ç¡®åŒ¹é…
            root_file = base_path / full_filename
            if root_file.is_file():
                logger.info(f"æ‰¾åˆ°é™„ä»¶æ–‡ä»¶ï¼ˆæ ¹ç›®å½•ç²¾ç¡®åŒ¹é…ï¼‰: {clean_filename} -> {root_file.name}")
                info = _build_file_info(base_path, root_file)
                info["type"] = asset_type  # æŒ‰æ˜ å°„å¼ºåˆ¶ç±»å‹
                return [info]
            else:
                logger.debug(f"  æ ¹ç›®å½•ç²¾ç¡®åŒ¹é…å¤±è´¥: {root_file} (å­˜åœ¨: {root_file.exists()}, æ˜¯æ–‡ä»¶: {root_file.is_file() if root_file.exists() else False})")

            # å…¨ç›®å½•ç²¾ç¡®åŒ¹é…
            matches = list(base_path.rglob(full_filename))
            logger.debug(f"  å…¨ç›®å½•ç²¾ç¡®åŒ¹é…: æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…é¡¹")
            for file_path in matches:
                if file_path.is_file():
                    logger.info(f"æ‰¾åˆ°é™„ä»¶æ–‡ä»¶ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰: {clean_filename} -> {file_path.name}")
                    info = _build_file_info(base_path, file_path)
                    info["type"] = asset_type
                    return [info]

            # ========== 2) æ¨¡ç³ŠåŒ¹é…ï¼ˆå‘½ä¸­æ–‡ä»¶å°±è¿”å› 1 æ¡ï¼‰ ==========
            pattern = f"*{clean_filename}*{ext}"

            root_matches = list(base_path.glob(pattern))
            matches = root_matches + list(base_path.rglob(pattern))

            # å»é‡
            seen = set()
            unique_matches = []
            for m in matches:
                if m not in seen:
                    seen.add(m)
                    unique_matches.append(m)
            matches = unique_matches

            if matches:
                best_match = None
                clean_lower = clean_filename.lower()

                for p in matches:
                    if p.is_file() and clean_lower in p.stem.lower():
                        best_match = p
                        break

                if not best_match:
                    for p in matches:
                        if p.is_file():
                            best_match = p
                            break

                if best_match and best_match.is_file():
                    logger.info(f"æ‰¾åˆ°é™„ä»¶æ–‡ä»¶ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰: {clean_filename} -> {best_match.name}")
                    info = _build_file_info(base_path, best_match)
                    info["type"] = asset_type
                    return [info]

    # ========== 3) æ–‡ä»¶å¤¹å‘½ä¸­ï¼šè¿”å›è¯¥æ–‡ä»¶å¤¹å†…æ‰€æœ‰æ–‡ä»¶ï¼ˆé€’å½’ï¼Œå¤šæ¡ï¼‰ ==========
    logger.debug(f"æ–‡ä»¶åŒ¹é…å¤±è´¥ï¼Œå°è¯•åŒ¹é…æ–‡ä»¶å¤¹åç§°: {clean_filename}")
    clean_lower = clean_filename.lower()

    matched_folders = []
    for folder_path in base_path.rglob("*"):
        if not folder_path.is_dir():
            continue
        folder_lower = folder_path.name.lower()

        # å®Œå…¨åŒ¹é…ä¼˜å…ˆ
        if clean_lower == folder_lower:
            matched_folders.insert(0, (folder_path, 1))
        # åŒ…å«åŒ¹é…å…œåº•
        elif clean_lower in folder_lower or folder_lower in clean_lower:
            matched_folders.append((folder_path, 2))

    matched_folders.sort(key=lambda x: x[1])

    for folder_path, priority in matched_folders:
        # é€’å½’æ”¶é›†æ–‡ä»¶å¤¹å†…æ‰€æœ‰æ–‡ä»¶
        all_files: List[Path] = []
        for p in folder_path.rglob("*"):
            if p.is_file() and not _should_skip_file(p):
                all_files.append(p)

        if not all_files:
            continue

        # æ’åºä¿è¯ç¨³å®šï¼ˆæ–¹ä¾¿æ¯”å¯¹å¯¼å…¥ç»“æœï¼‰
        all_files.sort(key=lambda p: str(p).lower())

        results = []
        for p in all_files:
            info = _build_file_info(base_path, p)
            results.append(info)

        match_type = "å®Œå…¨åŒ¹é…" if priority == 1 else "åŒ…å«åŒ¹é…"
        logger.info(
            f"æ‰¾åˆ°é™„ä»¶æ–‡ä»¶ï¼ˆæ–‡ä»¶å¤¹{match_type}ï¼‰: {clean_filename} -> æ–‡ä»¶å¤¹[{folder_path.name}] å…± {len(results)} ä¸ªæ–‡ä»¶"
        )
        return results

    # P1: æœªå‘½ä¸­æ—¶æ‰“å°"æ ¹ç›®å½•/å…¨ç›®å½•"å€™é€‰ stemï¼ˆé™åˆ¶ 5 ä¸ªï¼‰
    logger.warning(f"æœªæ‰¾åˆ°é™„ä»¶æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹: {filename} (æ¸…ç†å: {clean_filename}, æœç´¢è·¯å¾„: {base_path})")
    
    # åˆ—å‡ºæ ¹ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    try:
        root_files = [f.name for f in base_path.iterdir() if f.is_file()]
        logger.debug(f"  æ ¹ç›®å½•æ–‡ä»¶åˆ—è¡¨ï¼ˆå‰10ä¸ªï¼‰: {root_files[:10]}")
        
        # æŸ¥æ‰¾åŒ…å«å…³é”®å­—çš„æ–‡ä»¶
        root_candidates = []
        for file_path in base_path.iterdir():
            if file_path.is_file():
                stem_lower = file_path.stem.lower()
                clean_lower = clean_filename.lower()
                if clean_lower in stem_lower or stem_lower in clean_lower:
                    root_candidates.append(f"{file_path.stem}{file_path.suffix}")
                    if len(root_candidates) >= 5:
                        break
        if root_candidates:
            logger.info(f"  æ ¹ç›®å½•å€™é€‰æ–‡ä»¶: {root_candidates}")
        else:
            logger.debug(f"  æ ¹ç›®å½•æœªæ‰¾åˆ°åŒ…å« '{clean_filename}' çš„æ–‡ä»¶")
    except Exception as e:
        logger.debug(f"  æ— æ³•åˆ—å‡ºæ ¹ç›®å½•å€™é€‰æ–‡ä»¶: {str(e)}")
    
    # åˆ—å‡ºå…¨ç›®å½•ä¸‹å¯èƒ½çš„å€™é€‰æ–‡ä»¶ï¼ˆé™åˆ¶ 5 ä¸ªï¼‰
    try:
        all_candidates = []
        for file_path in base_path.rglob("*"):
            if file_path.is_file():
                stem_lower = file_path.stem.lower()
                clean_lower = clean_filename.lower()
                if clean_lower in stem_lower or stem_lower in clean_lower:
                    all_candidates.append(f"{file_path.stem}{file_path.suffix} (è·¯å¾„: {file_path.relative_to(base_path)})")
                    if len(all_candidates) >= 5:
                        break
        if all_candidates:
            logger.info(f"  å…¨ç›®å½•å€™é€‰æ–‡ä»¶: {all_candidates}")
        else:
            logger.debug(f"  å…¨ç›®å½•æœªæ‰¾åˆ°åŒ…å« '{clean_filename}' çš„æ–‡ä»¶")
    except Exception as e:
        logger.debug(f"  æ— æ³•åˆ—å‡ºå…¨ç›®å½•å€™é€‰æ–‡ä»¶: {str(e)}")
    
    return []


def find_column_by_variants(row: pd.Series, variants: List[str]) -> Optional[str]:
    """é€šè¿‡å¤šç§å˜ä½“æŸ¥æ‰¾åˆ—ï¼Œæ”¯æŒå¿½ç•¥ç©ºæ ¼å’Œå¤§å°å†™"""
    # å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    for col_name in variants:
        if col_name in row.index:
            val = row.get(col_name)
            # æ£€æŸ¥æ˜¯å¦ä¸º nan æˆ– None
            if pd.notna(val) and val is not None:
                val_str = str(val).strip()
                # æ’é™¤å­—ç¬¦ä¸² "nan"ï¼ˆpandas nan è½¬å­—ç¬¦ä¸²åçš„ç»“æœï¼‰
                if val_str and val_str.lower() != 'nan':
                    return val_str
    
    # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•å¿½ç•¥ç©ºæ ¼å’Œå¤§å°å†™çš„åŒ¹é…
    normalized_row_index = {str(col).strip().lower(): col for col in row.index}
    for variant in variants:
        normalized_variant = variant.strip().lower()
        if normalized_variant in normalized_row_index:
            col_name = normalized_row_index[normalized_variant]
            val = row.get(col_name)
            # æ£€æŸ¥æ˜¯å¦ä¸º nan æˆ– None
            if pd.notna(val) and val is not None:
                val_str = str(val).strip()
                # æ’é™¤å­—ç¬¦ä¸² "nan"ï¼ˆpandas nan è½¬å­—ç¬¦ä¸²åçš„ç»“æœï¼‰
                if val_str and val_str.lower() != 'nan':
                    return val_str
    return None


def map_excel_row_to_article(row: pd.Series, source_file_name: str, sheet_name: str, row_index: int) -> Optional[dict]:
    """
    å°† Excel è¡Œæ˜ å°„ä¸ºçŸ¥è¯†æ¡ç›® DTOï¼ˆå¿ å®è¿˜åŸåŸæ–‡æ¡£æ¨¡å¼ï¼‰
    
    è¡¨å¤´ï¼šåºå· | ç°è±¡ï¼ˆé—®é¢˜ï¼‰ | æ£€æŸ¥ç‚¹ï¼ˆåŸå› ï¼‰ | ç»´ä¿®å¯¹ç­–ï¼ˆè§£å†³åŠæ³•ï¼‰ | ç»´ä¿®è§†é¢‘ï¼ˆé™„ä»¶ï¼‰
    """
    # è¯»å–å­—æ®µï¼ˆåŸæ ·ä¿ç•™ï¼Œä¸ä¿®æ”¹ï¼‰
    # æ³¨æ„ï¼špandas è¯»å–æ—¶ï¼Œåˆ—åå¯èƒ½åŒ…å«å‰åç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦
    # æ”¯æŒå¤šç§åˆ—åæ ¼å¼ï¼ˆå¸¦æ‹¬å·å’Œä¸å¸¦æ‹¬å·ï¼‰
    serial_number = find_column_by_variants(row, ["åºå·"]) or ""
    
    phenomenon = find_column_by_variants(row, ["ç°è±¡ï¼ˆé—®é¢˜ï¼‰", "ç°è±¡(é—®é¢˜)", "ç°è±¡ ï¼ˆé—®é¢˜ï¼‰", "ç°è±¡ (é—®é¢˜)", "ç°è±¡", "é—®é¢˜", "æ•…éšœç°è±¡"]) or ""
    
    checkpoints = find_column_by_variants(row, ["æ£€æŸ¥ç‚¹ï¼ˆåŸå› ï¼‰", "æ£€æŸ¥ç‚¹(åŸå› )", "æ£€æŸ¥ç‚¹ ï¼ˆåŸå› ï¼‰", "æ£€æŸ¥ç‚¹ (åŸå› )", "æ£€æŸ¥ç‚¹", "åŸå› "]) or ""
    
    solution = find_column_by_variants(row, ["ç»´ä¿®å¯¹ç­–ï¼ˆè§£å†³åŠæ³•ï¼‰", "ç»´ä¿®å¯¹ç­–(è§£å†³åŠæ³•)", "ç»´ä¿®å¯¹ç­– ï¼ˆè§£å†³åŠæ³•ï¼‰", "ç»´ä¿®å¯¹ç­– (è§£å†³åŠæ³•)", "å¯¹ç­–", "ç»´ä¿®å¯¹ç­–", "è§£å†³åŠæ³•", "è§£å†³æ–¹æ³•"]) or ""
    
    video_reference = find_column_by_variants(row, ["ç»´ä¿®è§†é¢‘ï¼ˆé™„ä»¶ï¼‰", "ç»´ä¿®è§†é¢‘(é™„ä»¶)", "ç»´ä¿®è§†é¢‘ ï¼ˆé™„ä»¶ï¼‰", "ç»´ä¿®è§†é¢‘ (é™„ä»¶)", "ç»´ä¿®è§†é¢‘", "é™„ä»¶", "å¤‡æ³¨"]) or ""
    
    # è·³è¿‡ç©ºè¡Œå’Œæ ‡é¢˜è¡Œï¼ˆå¿…é¡»æœ‰ç°è±¡ï¼ˆé—®é¢˜ï¼‰ï¼‰
    # å¦‚æœåªæœ‰åºå·æˆ–å‹å·ç­‰éå¿…éœ€å­—æ®µï¼Œä¹Ÿè·³è¿‡ï¼ˆé¿å…å°†æ ‡é¢˜è¡Œæˆ–ç©ºè¡Œå½•å…¥æ•°æ®åº“ï¼‰
    if not phenomenon:
        return None
    
    # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœç°è±¡ï¼ˆé—®é¢˜ï¼‰çš„å€¼å°±æ˜¯åˆ—æ ‡é¢˜æœ¬èº«ï¼ˆå¦‚"ç°è±¡(é—®é¢˜)"ï¼‰ï¼Œåˆ™è·³è¿‡
    # é¿å…å°†æ ‡é¢˜è¡Œè¯¯å½“ä½œæ•°æ®è¡Œå½•å…¥
    title_keywords = ["ç°è±¡", "é—®é¢˜", "æ£€æŸ¥ç‚¹", "åŸå› ", "ç»´ä¿®å¯¹ç­–", "è§£å†³åŠæ³•", "ç»´ä¿®è§†é¢‘", "é™„ä»¶", "åºå·", "å‹å·"]
    if phenomenon in title_keywords or any(keyword in phenomenon for keyword in ["ç°è±¡ï¼ˆé—®é¢˜ï¼‰", "ç°è±¡(é—®é¢˜)", "æ£€æŸ¥ç‚¹ï¼ˆåŸå› ï¼‰", "ç»´ä¿®å¯¹ç­–ï¼ˆè§£å†³åŠæ³•ï¼‰"]):
        logger.debug(f"è·³è¿‡æ ‡é¢˜è¡Œæˆ–æ— æ•ˆè¡Œ: ç°è±¡={phenomenon}")
        return None
    
    # æ¸…ç†æ‰€æœ‰å­—æ®µä¸­çš„ä¸­æ‹¬å·æ ‡ç­¾ï¼ˆé˜²æ­¢Excelå•å…ƒæ ¼ä¸­æœ¬èº«å°±åŒ…å«è¿™äº›æ ‡ç­¾ï¼‰
    # åŒæ—¶å¤„ç† nan å€¼ï¼šå¦‚æœå­—æ®µå€¼æ˜¯ "nan" å­—ç¬¦ä¸²ï¼Œåˆ™æ¸…ç©º
    serial_number = clean_bracketed_labels(str(serial_number)) if serial_number and str(serial_number).lower() != 'nan' else ""
    phenomenon = clean_bracketed_labels(phenomenon) if phenomenon and str(phenomenon).lower() != 'nan' else ""
    checkpoints = clean_bracketed_labels(checkpoints) if checkpoints and str(checkpoints).lower() != 'nan' else ""
    solution = clean_bracketed_labels(solution) if solution and str(solution).lower() != 'nan' else ""
    video_reference = clean_bracketed_labels(video_reference) if video_reference and str(video_reference).lower() != 'nan' else ""
    
    # 1) title = åªä¿ç•™ç°è±¡ï¼ˆé—®é¢˜ï¼‰ï¼Œä¸æ·»åŠ å‹å·ä¿¡æ¯å’Œä¸­æ‹¬å·
    title = phenomenon.strip()
    # å†æ¬¡æ¸…ç†ï¼Œç¡®ä¿æ²¡æœ‰ä»»ä½•ä¸­æ‹¬å·æ ‡ç­¾
    title = clean_bracketed_labels(title)
    if not title:
        return None
    
    # 2) question_text = åªä¿ç•™ç°è±¡ï¼ˆé—®é¢˜ï¼‰çš„åŸå§‹å†…å®¹ï¼Œä¸æ·»åŠ åºå·ã€åˆ—æ ‡é¢˜å’Œä¸­æ‹¬å·
    # åºå·ä¸å½•å…¥æ•°æ®åº“ï¼Œåªä¿ç•™ç°è±¡å†…å®¹
    question_text = None
    if phenomenon and phenomenon.strip():
        cleaned_phenomenon = clean_bracketed_labels(phenomenon.strip())
        if cleaned_phenomenon:
            question_text = cleaned_phenomenon
    # æœ€ç»ˆæ¸…ç†ï¼Œç¡®ä¿æ²¡æœ‰ä»»ä½•ä¸­æ‹¬å·æ ‡ç­¾
    if question_text:
        question_text = clean_bracketed_labels(question_text)
    
    # 3) cause_text = åªä¿ç•™æ£€æŸ¥ç‚¹ï¼ˆåŸå› ï¼‰çš„åŸå§‹å†…å®¹ï¼Œä¸æ·»åŠ åˆ—æ ‡é¢˜å’Œä¸­æ‹¬å·
    cause_text = checkpoints.strip() if checkpoints and checkpoints.strip() else None
    # å†æ¬¡æ¸…ç†ï¼Œç¡®ä¿æ²¡æœ‰ä»»ä½•ä¸­æ‹¬å·æ ‡ç­¾
    if cause_text:
        cause_text = clean_bracketed_labels(cause_text)
    
    # 4) solution_text = ä¿ç•™ç»´ä¿®å¯¹ç­–ï¼ˆè§£å†³åŠæ³•ï¼‰å’Œç»´ä¿®è§†é¢‘ï¼ˆé™„ä»¶ï¼‰çš„åŸå§‹å†…å®¹ï¼Œä¸æ·»åŠ åˆ—æ ‡é¢˜å’Œä¸­æ‹¬å·
    # æ³¨æ„ï¼šç»´ä¿®è§†é¢‘ï¼ˆé™„ä»¶ï¼‰çš„å†…å®¹ä¼šæ·»åŠ åˆ° solution_text ä¸­ï¼Œè¿™æ ·å³ä½¿æ²¡æœ‰åŒ¹é…åˆ°é™„ä»¶æ–‡ä»¶ï¼Œç”¨æˆ·ä¹Ÿèƒ½çœ‹åˆ°å‚è€ƒä¿¡æ¯
    # åŒæ—¶ï¼Œå¦‚æœåŒ¹é…åˆ°é™„ä»¶æ–‡ä»¶ï¼Œä¹Ÿä¼šåˆ›å»ºé™„ä»¶è®°å½•ï¼Œåœ¨é™„ä»¶éƒ¨åˆ†æ˜¾ç¤º
    solution_parts = []
    if solution and solution.strip():
        cleaned_solution = clean_bracketed_labels(solution.strip())
        if cleaned_solution:
            solution_parts.append(cleaned_solution)
    
    # 5) ç»´ä¿®è§†é¢‘ï¼ˆé™„ä»¶ï¼‰ï¼šæ·»åŠ åˆ° solution_text ä¸­ï¼ˆå³ä½¿æ²¡æœ‰åŒ¹é…åˆ°é™„ä»¶æ–‡ä»¶ï¼Œç”¨æˆ·ä¹Ÿèƒ½çœ‹åˆ°å‚è€ƒä¿¡æ¯ï¼‰
    if video_reference and video_reference.strip():
        cleaned_video = clean_bracketed_labels(video_reference.strip())
        if cleaned_video:
            solution_parts.append(cleaned_video)
    
    solution_text = "\n\n".join(solution_parts) if solution_parts else None
    # æœ€ç»ˆæ¸…ç†ï¼Œç¡®ä¿æ²¡æœ‰ä»»ä½•ä¸­æ‹¬å·æ ‡ç­¾
    if solution_text:
        solution_text = clean_bracketed_labels(solution_text)
    
    # 6) scope_json å¿…é¡»åŒ…å«ï¼šè®¾å¤‡ç³»åˆ—ã€æ¥æºæ–‡ä»¶ã€sheetã€è¡Œå·
    scope_data = {
        "è®¾å¤‡ç³»åˆ—": "YH400/YH500",
        "æ¥æºæ–‡ä»¶": source_file_name,
        "sheet": sheet_name,
        "è¡Œå·": row_index
    }
    scope_json = json.dumps(scope_data, ensure_ascii=False)
    
    # 7) tags è‡³å°‘åŒ…å«ï¼šYH400, YH500, æ¥æº:{æ–‡ä»¶å}
    tags = ["YH400", "YH500"]
    file_name_without_ext = Path(source_file_name).stem
    if file_name_without_ext:
        tags.append(f"æ¥æº:{file_name_without_ext}")
    
    # å¦‚æœç°è±¡/åŸå› ä¸­åŒ…å«å…³é”®å­—ä¹Ÿå¯è¿½åŠ ï¼ˆç®€å•å…³é”®è¯æå–ï¼‰
    text_content = f"{phenomenon} {checkpoints} {solution}".lower()
    if "yh400" in text_content and "YH400" not in tags:
        # å·²ç»åœ¨tagsä¸­ï¼Œè·³è¿‡
        pass
    if "yh500" in text_content and "YH500" not in tags:
        # å·²ç»åœ¨tagsä¸­ï¼Œè·³è¿‡
        pass
    
    # æå–é™„ä»¶ä¿¡æ¯ï¼ˆç”¨äºåç»­åˆ›å»º kb_assetï¼‰
    # æ”¯æŒä¸€ä¸ªå•å…ƒæ ¼åŒ…å«å¤šä¸ªå¼•ç”¨ï¼ˆç”¨æ¢è¡Œç¬¦ã€åˆ†å·ç­‰åˆ†éš”ï¼‰ï¼Œæ¯ä¸ªå¼•ç”¨éƒ½å°è¯•åŒ¹é…å¹¶åˆ›å»ºé™„ä»¶è®°å½•
    attachment_info_list = []  # æ”¹ä¸ºåˆ—è¡¨ï¼Œæ”¯æŒå¤šä¸ªé™„ä»¶
    has_attachment_reference = False  # æ ‡è®°æ˜¯å¦æœ‰é™„ä»¶å¼•ç”¨ï¼ˆæ— è®ºæ˜¯å¦æ‰¾åˆ°æ–‡ä»¶ï¼‰
    if video_reference:
        has_attachment_reference = True
        # å…ˆæŒ‰æ¢è¡Œç¬¦åˆ†å‰²ï¼Œç„¶åå¯¹æ¯ä¸€è¡Œå†æŒ‰åˆ†å·ã€ä¸­æ–‡åˆ†å·ç­‰åˆ†å‰²
        # è¿™æ ·å¯ä»¥å¤„ç†å¤šç§æ ¼å¼ï¼šæ¢è¡Œåˆ†éš”ã€åˆ†å·åˆ†éš”ã€æˆ–æ··åˆ
        all_references = []
        for line in video_reference.split('\n'):
            line = line.strip()
            if not line:
                continue
            # æŒ‰åˆ†å·ã€ä¸­æ–‡åˆ†å·åˆ†å‰²
            parts = re.split(r'[ï¼›;]', line)
            for part in parts:
                part = part.strip()
                if part:
                    all_references.append(part)
        
        # P0: å¦‚æœä¸€æ®µé‡Œå‡ºç°å¤šä¸ª"å‚è€ƒ"ï¼Œå¼ºåˆ¶äºŒæ¬¡æ­£åˆ™æå–å¹¶å±•å¼€
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ª"å‚è€ƒ"å…³é”®å­—
        reference_count = video_reference.count('å‚è€ƒ')
        if reference_count > 1 or len(all_references) == 1:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰"å‚è€ƒ"xxx""æ ¼å¼çš„å¼•ç”¨ï¼ˆæ”¯æŒä¸­æ–‡å¼•å· "" å’Œè‹±æ–‡å¼•å· ""ï¼‰
            # æ­£åˆ™ï¼šå‚è€ƒ[""]... [""]ï¼ˆæ”¯æŒä¸­æ–‡å¼•å·å’Œè‹±æ–‡å¼•å·ï¼‰
            pattern = r'å‚è€ƒ["""""]([^"""""]+)["""""]'
            matches = re.findall(pattern, video_reference)
            if matches:
                # å¦‚æœæ­£åˆ™æå–åˆ°æ›´å¤šå¼•ç”¨ï¼Œä½¿ç”¨æ­£åˆ™æå–çš„ç»“æœ
                if len(matches) > len(all_references):
                    all_references = [f'å‚è€ƒ"{m}"' for m in matches]
                    logger.debug(f"é€šè¿‡æ­£åˆ™æå–åˆ° {len(all_references)} ä¸ªå¼•ç”¨ï¼ˆåŸåˆ†å‰²ç»“æœ: {len(all_references) - len(matches) + len(matches)} ä¸ªï¼‰")
                elif len(matches) == len(all_references) and reference_count > 1:
                    # å³ä½¿æ•°é‡ç›¸åŒï¼Œå¦‚æœæ£€æµ‹åˆ°å¤šä¸ª"å‚è€ƒ"ï¼Œä¹Ÿä½¿ç”¨æ­£åˆ™æå–çš„ç»“æœï¼ˆæ›´å‡†ç¡®ï¼‰
                    all_references = [f'å‚è€ƒ"{m}"' for m in matches]
                    logger.debug(f"æ£€æµ‹åˆ°å¤šä¸ª'å‚è€ƒ'ï¼Œä½¿ç”¨æ­£åˆ™æå–ç»“æœ: {len(all_references)} ä¸ªå¼•ç”¨")
        
        if len(all_references) > 1:
            logger.info(f"æ£€æµ‹åˆ° {len(all_references)} ä¸ªå¼•ç”¨ï¼Œå°†å°è¯•åŒ¹é…æ‰€æœ‰å¼•ç”¨")
        else:
            logger.debug(f"æå–åˆ° {len(all_references)} ä¸ªå¼•ç”¨: {all_references}")
        
        matched_count = 0
        for idx, ref_line in enumerate(all_references, 1):
            extracted_filename = extract_filename_from_reference(ref_line)
            if extracted_filename:
                # å†æ¬¡æ¸…ç†æ–‡ä»¶åï¼ˆç¡®ä¿å»é™¤æ‰€æœ‰å¼•å·ï¼ŒåŒ…æ‹¬å…¨è§’å•å¼•å· ''ï¼‰
                clean_extracted = extracted_filename.strip('"""\'""\'ã€Šã€‹ã€ã€‘[]()ï¼ˆï¼‰').strip()
                if not clean_extracted:
                    logger.debug(f"å¼•ç”¨ {idx}/{len(all_references)}: æ¸…ç†åæ–‡ä»¶åä¸ºç©º (åŸå§‹: {ref_line[:50]}...)")
                    continue
                
                file_infos = find_attachment_files(clean_extracted)
                if file_infos:
                    # æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯å•ä¸ªæ–‡ä»¶æˆ–å¤šä¸ªæ–‡ä»¶ï¼‰ï¼Œå…¨éƒ¨æ·»åŠ åˆ°åˆ—è¡¨
                    for file_info in file_infos:
                        attachment_info_list.append({
                            "filename": clean_extracted,
                            "file_name": file_info.get("file_name", os.path.basename(file_info["path"])),
                            "url": file_info["url"],
                            "asset_type": file_info["type"],
                            "size": file_info["size"],
                            "relative_path": file_info.get("relative_path"),
                            "source_ref": clean_extracted,  # Excel å¼•ç”¨åï¼ˆç”¨äºæº¯æºï¼‰
                            "source_folder": Path(file_info["path"]).parent.name if file_info.get("path") else None
                        })
                    matched_count += len(file_infos)
                    if len(file_infos) == 1:
                        logger.info(f"âœ“ [{idx}/{len(all_references)}] æ‰¾åˆ°é™„ä»¶: {clean_extracted} -> {file_infos[0]['url']}")
                    else:
                        logger.info(f"âœ“ [{idx}/{len(all_references)}] æ‰¾åˆ°é™„ä»¶ï¼ˆæ–‡ä»¶å¤¹ï¼‰: {clean_extracted} -> {len(file_infos)} ä¸ªæ–‡ä»¶")
                else:
                    logger.warning(f"âœ— [{idx}/{len(all_references)}] æœªæ‰¾åˆ°é™„ä»¶: {clean_extracted} (åŸå§‹å¼•ç”¨: {ref_line[:50]}...)")
            else:
                logger.debug(f"å¼•ç”¨ {idx}/{len(all_references)}: æ— æ³•æå–æ–‡ä»¶å (åŸå§‹: {ref_line[:50]}...)")
        
        if len(all_references) > 1:
            # P1: åŒ¹é…ç»Ÿè®¡æ‹†æˆ "å‘½ä¸­å¼•ç”¨æ•° / æ€»å¼•ç”¨æ•° + æ–‡ä»¶æ€»æ•°"
            total_files = len(attachment_info_list)
            logger.info(f"å¤šå¼•ç”¨åŒ¹é…ç»“æœ: {matched_count}/{len(all_references)} ä¸ªå¼•ç”¨æ‰¾åˆ°æ–‡ä»¶ï¼Œå…± {total_files} ä¸ªæ–‡ä»¶")
    
    # _attachment_info å§‹ç»ˆæ˜¯åˆ—è¡¨ï¼ˆç»Ÿä¸€å¤„ç†ï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜ï¼‰
    attachment_info = attachment_info_list if attachment_info_list else None
    
    # å»é‡ï¼šåŸºäº URL æˆ– relative_path å»é‡ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
    if attachment_info:
        seen_urls = set()
        unique_attachments = []
        for att in attachment_info:
            url_key = att.get("url") or att.get("relative_path")
            if url_key and url_key not in seen_urls:
                seen_urls.add(url_key)
                unique_attachments.append(att)
        attachment_info = unique_attachments if unique_attachments else None
    
    # æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æ²¡æœ‰ä¸­æ‹¬å·æ ‡ç­¾ï¼ˆæœ€åä¸€æ¬¡æ¸…ç†ï¼‰
    final_title = clean_bracketed_labels(title) if title else ""
    final_question_text = clean_bracketed_labels(question_text) if question_text else None
    final_cause_text = clean_bracketed_labels(cause_text) if cause_text else None
    final_solution_text = clean_bracketed_labels(solution_text) if solution_text else None
    
    # è®°å½•æ¸…ç†æ—¥å¿—ï¼ˆä»…åœ¨å‰å‡ æ¬¡å¯¼å…¥æ—¶è®°å½•ï¼Œé¿å…æ—¥å¿—è¿‡å¤šï¼‰
    if row_index <= 3:
        logger.info(f"[è¡Œ {row_index}] æ¸…ç†å‰ -> æ¸…ç†å:")
        logger.info(f"  title: {title[:50]}... -> {final_title[:50]}...")
        if question_text:
            logger.info(f"  questionText: {question_text[:50]}... -> {final_question_text[:50] if final_question_text else 'None'}...")
        if cause_text:
            logger.info(f"  causeText: {cause_text[:50]}... -> {final_cause_text[:50] if final_cause_text else 'None'}...")
        if solution_text:
            logger.info(f"  solutionText: {solution_text[:50]}... -> {final_solution_text[:50] if final_solution_text else 'None'}...")
    
    return {
        "title": final_title,
        "questionText": final_question_text,
        "causeText": final_cause_text,
        "solutionText": final_solution_text,
        "scopeJson": scope_json,
        "tags": ", ".join(tags),
        "createdBy": "ç³»ç»Ÿå¯¼å…¥",
        "_attachment_info": attachment_info,  # å†…éƒ¨å­—æ®µï¼Œå§‹ç»ˆæ˜¯åˆ—è¡¨æˆ– Noneï¼Œä¸å‘é€ç»™åç«¯
        "_has_attachment_reference": has_attachment_reference  # æ ‡è®°æ˜¯å¦æœ‰é™„ä»¶å¼•ç”¨ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
    }


@app.post("/import/excel", response_model=ExcelImportResponse)
async def import_excel(file: UploadFile = File(...)):
    """
    å¯¼å…¥ Excel æ–‡ä»¶ä¸ºçŸ¥è¯†æ¡ç›®
    
    - æ¥æ”¶ .xlsx æ–‡ä»¶
    - ä½¿ç”¨ pandas è¯»å–
    - æ¯è¡Œæ˜ å°„ä¸ºä¸€æ¡çŸ¥è¯†è‰ç¨¿
    - è°ƒç”¨ .NET çš„ /api/ai/kb/articles/batch åˆ›å»ºè‰ç¨¿
    - å¦‚æœæ‰¾åˆ°é™„ä»¶æ–‡ä»¶ï¼Œåˆ›å»º kb_asset è®°å½•ï¼ˆæ–¹æ¡ˆ Bï¼šå…ƒæ•°æ®å…³è”ï¼‰
    """
    logger.info(f"æ”¶åˆ° Excel å¯¼å…¥è¯·æ±‚: {file.filename}")
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    if not file.filename or not file.filename.endswith('.xlsx'):
        logger.warning(f"æ–‡ä»¶ç±»å‹ä¸æ­£ç¡®: {file.filename}")
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ .xlsx æ ¼å¼çš„ Excel æ–‡ä»¶")
    
    try:
        # è¯»å– Excel æ–‡ä»¶
        contents = await file.read()
        # å°† bytes è½¬æ¢ä¸º BytesIO å¯¹è±¡ï¼Œpandas æ‰èƒ½è¯»å–
        excel_file = BytesIO(contents)
        
        # å…ˆå°è¯•ä»ç¬¬ä¸€è¡Œè¯»å–è¡¨å¤´
        df = pd.read_excel(excel_file, engine='openpyxl', header=0)
        
        # éªŒè¯å¿…éœ€å­—æ®µï¼ˆæ”¯æŒå¤šç§åˆ—åï¼‰
        # æ•…éšœç°è±¡åˆ—åå¯èƒ½ä¸ºï¼šæ•…éšœç°è±¡ã€ç°è±¡ï¼ˆé—®é¢˜ï¼‰ã€ç°è±¡ã€é—®é¢˜
        fault_phenomenon_columns = ["æ•…éšœç°è±¡", "ç°è±¡ï¼ˆé—®é¢˜ï¼‰", "ç°è±¡(é—®é¢˜)", "ç°è±¡ ï¼ˆé—®é¢˜ï¼‰", "ç°è±¡ (é—®é¢˜)", "ç°è±¡", "é—®é¢˜"]
        
        # æ”¹è¿›çš„åˆ—ååŒ¹é…ï¼šæ”¯æŒå¿½ç•¥ç©ºæ ¼ã€å¤§å°å†™ç­‰
        def normalize_column_name(col_name: str) -> str:
            """æ ‡å‡†åŒ–åˆ—åï¼šå»é™¤å‰åç©ºæ ¼ï¼Œç»Ÿä¸€å¤§å°å†™"""
            return str(col_name).strip().lower()
        
        # æ ‡å‡†åŒ–æ‰€æœ‰åˆ—åå’ŒåŒ¹é…åˆ—è¡¨
        normalized_columns = {normalize_column_name(col): col for col in df.columns}
        normalized_fault_phenomenon = [normalize_column_name(col) for col in fault_phenomenon_columns]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„åˆ—
        has_fault_phenomenon = any(norm_col in normalized_columns for norm_col in normalized_fault_phenomenon)
        header_row = 0  # è®°å½• header è¡Œå·
        
        logger.info(f"ç¬¬ä¸€è¡Œè¯»å–åˆ°çš„åˆ—å: {list(df.columns)}")
        logger.info(f"æ ‡å‡†åŒ–åçš„åˆ—å: {list(normalized_columns.keys())}")
        logger.info(f"æœŸæœ›çš„æ ‡å‡†åŒ–åˆ—å: {normalized_fault_phenomenon}")
        logger.info(f"åŒ¹é…ç»“æœ: {has_fault_phenomenon}")
        
        # å¦‚æœç¬¬ä¸€è¡Œæ²¡æœ‰æ‰¾åˆ°å¿…éœ€å­—æ®µï¼Œå°è¯•ä»ç¬¬äºŒè¡Œè¯»å–ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰
        if not has_fault_phenomenon:
            logger.info("ç¬¬ä¸€è¡Œæœªæ‰¾åˆ°å¿…éœ€å­—æ®µï¼Œå°è¯•ä»ç¬¬äºŒè¡Œè¯»å–è¡¨å¤´ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰")
            excel_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            df = pd.read_excel(excel_file, engine='openpyxl', header=1)
            header_row = 1  # æ›´æ–° header è¡Œå·
            
            # é‡æ–°æ ‡å‡†åŒ–åˆ—å
            normalized_columns = {normalize_column_name(col): col for col in df.columns}
            normalized_fault_phenomenon = [normalize_column_name(col) for col in fault_phenomenon_columns]
            logger.info(f"ç¬¬äºŒè¡Œè¯»å–åˆ°çš„åˆ—å: {list(df.columns)}")
            logger.info(f"æ ‡å‡†åŒ–åçš„åˆ—å: {list(normalized_columns.keys())}")
            logger.info(f"æœŸæœ›çš„æ ‡å‡†åŒ–åˆ—å: {normalized_fault_phenomenon}")
            
            # å†æ¬¡éªŒè¯
            has_fault_phenomenon = any(norm_col in normalized_columns for norm_col in normalized_fault_phenomenon)
            logger.info(f"åŒ¹é…ç»“æœ: {has_fault_phenomenon}")
            
            # å¦‚æœä»ç„¶æ²¡æœ‰åŒ¹é…ï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…ï¼ˆåªæ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®å­—ï¼‰
            if not has_fault_phenomenon:
                logger.info("å°è¯•æ›´å®½æ¾çš„åŒ¹é…ï¼šæ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«'ç°è±¡'æˆ–'é—®é¢˜'å…³é”®å­—")
                for col in df.columns:
                    col_lower = str(col).strip().lower()
                    if 'ç°è±¡' in col_lower or 'é—®é¢˜' in col_lower or 'æ•…éšœ' in col_lower:
                        logger.info(f"æ‰¾åˆ°å¯èƒ½çš„åŒ¹é…åˆ—: '{col}' (æ ‡å‡†åŒ–: '{col_lower}')")
                        has_fault_phenomenon = True
                        break
            
            if not has_fault_phenomenon:
                # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                error_msg = (
                    f"ç¼ºå°‘å¿…éœ€å­—æ®µ: è¯·åŒ…å«ä»¥ä¸‹ä»»ä¸€åˆ—å - {', '.join(fault_phenomenon_columns)}\n"
                    f"å®é™…è¯»å–åˆ°çš„åˆ—å: {', '.join(df.columns)}\n"
                    f"æç¤º: è¯·ç¡®ä¿Excelæ–‡ä»¶åŒ…å«'ç°è±¡ï¼ˆé—®é¢˜ï¼‰'ã€'ç°è±¡(é—®é¢˜)'ã€'ç°è±¡ (é—®é¢˜)'æˆ–'ç°è±¡'åˆ—"
                )
                logger.error(error_msg)
                raise HTTPException(status_code=400, detail=error_msg)
        
        if df.empty:
            raise HTTPException(status_code=400, detail="Excel æ–‡ä»¶ä¸ºç©º")
        
        logger.info(f"è¯»å–åˆ° {len(df)} è¡Œæ•°æ®")
        logger.info(f"æœ€ç»ˆä½¿ç”¨çš„åˆ—å: {list(df.columns)}")
        
        # æ˜ç¡®è¿‡æ»¤æ‰"å‹å·"åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œé¿å…å½•å…¥æ•°æ®åº“
        columns_to_drop = []
        for col in df.columns:
            col_str = str(col).strip()
            # å¦‚æœåˆ—ååŒ…å«"å‹å·"ï¼Œåˆ™æ ‡è®°ä¸ºåˆ é™¤
            if "å‹å·" in col_str:
                columns_to_drop.append(col)
                logger.debug(f"æ£€æµ‹åˆ°'å‹å·'åˆ—ï¼Œå°†å¿½ç•¥: {col_str}")
        
        if columns_to_drop:
            df = df.drop(columns=columns_to_drop)
            logger.info(f"å·²è¿‡æ»¤ {len(columns_to_drop)} ä¸ª'å‹å·'ç›¸å…³åˆ—")
        
        # è·å– sheet åç§°ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        sheet_name = "Sheet1"  # é»˜è®¤å€¼
        try:
            # å°è¯•ä» Excel æ–‡ä»¶ä¸­è·å– sheet åç§°
            excel_file.seek(0)
            import openpyxl
            wb = openpyxl.load_workbook(excel_file, read_only=True)
            if wb.sheetnames:
                sheet_name = wb.sheetnames[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ª sheet çš„åç§°
            excel_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        except Exception as e:
            logger.warning(f"æ— æ³•è·å– sheet åç§°ï¼Œä½¿ç”¨é»˜è®¤å€¼: {str(e)}")
        
        # ç¡®å®š header è¡Œå·ï¼ˆç”¨äºè®¡ç®— Excel è¡Œå·ï¼‰
        header_row = 0 if has_fault_phenomenon else 1
        
        # æ˜ å°„æ¯è¡Œä¸ºçŸ¥è¯†æ¡ç›® DTO
        articles = []  # å‘é€ç»™åç«¯çš„æ–‡ç« ï¼ˆä¸åŒ…å«å†…éƒ¨å­—æ®µï¼‰
        articles_with_attachments = []  # åŒ…å«é™„ä»¶ä¿¡æ¯çš„å®Œæ•´æ–‡ç« æ•°æ®
        failures = []
        skipped_rows = 0
        attachment_match_count = 0  # é™„ä»¶åŒ¹é…ç»Ÿè®¡
        attachment_not_found_count = 0  # é™„ä»¶æœªæ‰¾åˆ°ç»Ÿè®¡
        
        for idx, row in df.iterrows():
            try:
                # Excel è¡Œå· = pandas index + headerè¡Œæ•° + 1ï¼ˆExcelä»1å¼€å§‹è®¡æ•°ï¼‰
                # å¦‚æœ header=0ï¼Œåˆ™ Excelè¡Œå· = idx + 2ï¼ˆç¬¬1è¡Œæ˜¯è¡¨å¤´ï¼Œç¬¬2è¡Œå¼€å§‹æ˜¯æ•°æ®ï¼‰
                # å¦‚æœ header=1ï¼Œåˆ™ Excelè¡Œå· = idx + 3ï¼ˆç¬¬1è¡Œæ˜¯æ ‡é¢˜ï¼Œç¬¬2è¡Œæ˜¯è¡¨å¤´ï¼Œç¬¬3è¡Œå¼€å§‹æ˜¯æ•°æ®ï¼‰
                excel_row_number = int(idx) + header_row + 2
                
                # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€è¡Œçš„æ•°æ®
                if idx == 0:
                    logger.info(f"ç¬¬ä¸€è¡Œæ•°æ®é¢„è§ˆ: {dict(row)}")
                
                article = map_excel_row_to_article(row, file.filename, sheet_name, excel_row_number)
                if article:
                    # ä¿å­˜åŒ…å«é™„ä»¶ä¿¡æ¯çš„åŸå§‹ article
                    articles_with_attachments.append(article)
                    # ç§»é™¤å†…éƒ¨å­—æ®µï¼ˆä¸å‘é€ç»™åç«¯ï¼‰
                    article_for_api = {k: v for k, v in article.items() if not k.startswith("_")}
                    articles.append(article_for_api)
                else:
                    skipped_rows += 1
                    if idx < 3:  # åªè®°å½•å‰3è¡Œçš„è·³è¿‡åŸå› 
                        logger.info(f"ç¬¬ {excel_row_number} è¡Œè¢«è·³è¿‡ï¼ˆç©ºè¡Œæˆ–ç¼ºå°‘å¿…éœ€å­—æ®µï¼‰")
                # å¦‚æœè¿”å› Noneï¼Œè¯´æ˜æ˜¯ç©ºè¡Œï¼Œè·³è¿‡
            except Exception as e:
                excel_row_number = int(idx) + header_row + 2
                logger.error(f"å¤„ç†ç¬¬ {excel_row_number} è¡Œæ—¶å‡ºé”™: {str(e)}")
                failures.append({
                    "row_index": excel_row_number,
                    "reason": str(e)
                })
        
        # ç»Ÿè®¡é™„ä»¶åŒ¹é…æƒ…å†µï¼ˆæ”¯æŒå¤šä¸ªé™„ä»¶ï¼‰
        attachment_match_count = 0
        for a in articles_with_attachments:
            att_info = a.get("_attachment_info")
            if att_info:
                # _attachment_info ç°åœ¨å§‹ç»ˆæ˜¯åˆ—è¡¨
                if isinstance(att_info, list):
                    attachment_match_count += len(att_info)
                else:
                    # å…¼å®¹æ—§æ•°æ®
                    attachment_match_count += 1
        
        attachment_total = sum(1 for a in articles_with_attachments if a.get("_has_attachment_reference", False))
        attachment_not_found_count = attachment_total - sum(1 for a in articles_with_attachments if a.get("_attachment_info"))
        
        logger.info(f"æˆåŠŸæ˜ å°„ {len(articles)} æ¡ï¼Œè·³è¿‡ {skipped_rows} è¡Œï¼Œå¤±è´¥ {len(failures)} è¡Œ")
        if attachment_total > 0:
            match_rate = (attachment_match_count / attachment_total * 100) if attachment_total > 0 else 0
            logger.info(f"ğŸ“ é™„ä»¶åŒ¹é…ç»Ÿè®¡: æ‰¾åˆ° {attachment_match_count}/{attachment_total} ä¸ª ({match_rate:.1f}%)ï¼Œæœªæ‰¾åˆ° {attachment_not_found_count} ä¸ª")
        
        if not articles:
            error_msg = f"æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®è¡Œã€‚æ€»è¡Œæ•°: {len(df)}, è·³è¿‡: {skipped_rows}, å¤±è´¥: {len(failures)}"
            if len(df) > 0:
                error_msg += f"\nåˆ—å: {list(df.columns)}"
                error_msg += f"\nç¬¬ä¸€è¡Œæ•°æ®: {dict(df.iloc[0]) if len(df) > 0 else 'N/A'}"
            raise HTTPException(status_code=400, detail=error_msg)
        
        # è°ƒç”¨ .NET åç«¯æ‰¹é‡åˆ›å»ºæ¥å£
        logger.info(f"å‡†å¤‡è°ƒç”¨ .NET åç«¯: {DOTNET_BASE_URL}/api/ai/kb/articles/batch, æ–‡ç« æ•°é‡: {len(articles)}")
        
        async with httpx.AsyncClient() as client:
            try:
                response = await client.post(
                    f"{DOTNET_BASE_URL}/api/ai/kb/articles/batch",
                    json={"articles": articles},
                    headers={
                        "Content-Type": "application/json",
                        "X-Tenant-Id": DEFAULT_TENANT,
                        "X-Internal-Token": INTERNAL_TOKEN
                    },
                    timeout=30.0
                )
                
                logger.info(f".NET åç«¯å“åº”çŠ¶æ€ç : {response.status_code}")
                
                if response.status_code != 200:
                    error_text = response.text
                    logger.error(f".NET åç«¯è¿”å›é”™è¯¯: {error_text}")
                    raise HTTPException(
                        status_code=response.status_code,
                        detail=f".NET åç«¯è¿”å›é”™è¯¯: {error_text}"
                    )
            except httpx.ConnectError as e:
                logger.error(f"æ— æ³•è¿æ¥åˆ° .NET åç«¯ {DOTNET_BASE_URL}: {str(e)}")
                raise HTTPException(
                    status_code=503,
                    detail=f"æ— æ³•è¿æ¥åˆ° .NET åç«¯æœåŠ¡ï¼Œè¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨: {DOTNET_BASE_URL}"
                )
            except httpx.TimeoutException as e:
                logger.error(f"è°ƒç”¨ .NET åç«¯è¶…æ—¶: {str(e)}")
                raise HTTPException(
                    status_code=504,
                    detail="è°ƒç”¨ .NET åç«¯è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
                )
            
            result = response.json()
            
            # å¤„ç†è¿”å›ç»“æœ
            success_count = result.get("successCount", 0)
            failure_count = result.get("failureCount", 0)
            article_ids = []
            article_id_to_attachment = {}  # æ˜ å°„ article_id -> attachment_info
            
            # æ”¶é›†æˆåŠŸåˆ›å»ºçš„ article IDs å’Œé™„ä»¶ä¿¡æ¯
            # æ”¯æŒä¸€ä¸ªæ–‡ç« æœ‰å¤šä¸ªé™„ä»¶ï¼ˆ_attachment_info å¯èƒ½æ˜¯å•ä¸ªå¯¹è±¡æˆ–åˆ—è¡¨ï¼‰
            article_id_to_attachments = {}  # æ”¹ä¸ºæ˜ å°„åˆ°é™„ä»¶åˆ—è¡¨
            for i, item in enumerate(result.get("results", [])):
                if item.get("success") and item.get("articleId"):
                    article_id = item["articleId"]
                    article_ids.append(article_id)
                    
                    # å¦‚æœå¯¹åº”çš„ article æœ‰é™„ä»¶ä¿¡æ¯ï¼Œä¿å­˜æ˜ å°„
                    if i < len(articles_with_attachments):
                        att_info = articles_with_attachments[i].get("_attachment_info")
                        if att_info:
                            # _attachment_info ç°åœ¨å§‹ç»ˆæ˜¯åˆ—è¡¨ï¼ˆç»Ÿä¸€å¤„ç†ï¼‰
                            if isinstance(att_info, list):
                                article_id_to_attachments[article_id] = att_info
                            else:
                                # å…¼å®¹æ—§æ•°æ®ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‡ºç°ï¼Œä½†ä¿é™©èµ·è§ï¼‰
                                article_id_to_attachments[article_id] = [att_info]
                elif not item.get("success"):
                    # è®°å½•åç«¯è¿”å›çš„å¤±è´¥ä¿¡æ¯
                    failures.append({
                        "row_index": item.get("index", -1) + (header_row + 2),  # è½¬æ¢ä¸º Excel è¡Œå·
                        "reason": item.get("error", "æœªçŸ¥é”™è¯¯")
                    })
            
            # æ‰¹é‡åˆ›å»ºé™„ä»¶ï¼ˆæ–¹æ¡ˆ Bï¼šå…ƒæ•°æ®å…³è”ï¼‰
            if article_id_to_attachments:
                total_attachments = sum(len(atts) for atts in article_id_to_attachments.values())
                logger.info(f"å‡†å¤‡åˆ›å»º {total_attachments} ä¸ªé™„ä»¶è®°å½•ï¼ˆæ¶‰åŠ {len(article_id_to_attachments)} ç¯‡æ–‡ç« ï¼‰")
                
                # è®°å½•æ¯ä¸ªæ–‡ç« çš„é™„ä»¶æ•°é‡
                for article_id, att_info_list in article_id_to_attachments.items():
                    logger.info(f"  æ–‡ç«  ID {article_id}: {len(att_info_list)} ä¸ªé™„ä»¶")
                
                assets_to_create = []
                
                for article_id, att_info_list in article_id_to_attachments.items():
                    # ä¸ºæ¯ä¸ªé™„ä»¶åˆ›å»ºè®°å½•
                    for att_info in att_info_list:
                        assets_to_create.append({
                            "articleId": article_id,
                            "assetType": att_info["asset_type"],
                            "fileName": att_info["file_name"],
                            "url": att_info["url"],
                            "size": att_info["size"],
                            "duration": None  # è§†é¢‘æ—¶é•¿æš‚ä¸æ”¯æŒè‡ªåŠ¨è·å–
                        })
                
                if assets_to_create:
                    try:
                        asset_response = await client.post(
                            f"{DOTNET_BASE_URL}/api/ai/kb/articles/assets/batch",
                            json={"assets": assets_to_create},
                            headers={
                                "Content-Type": "application/json",
                                "X-Tenant-Id": DEFAULT_TENANT,
                                "X-Internal-Token": INTERNAL_TOKEN
                            },
                            timeout=30.0
                        )
                        
                        if asset_response.status_code == 200:
                            asset_result = asset_response.json()
                            success_count = asset_result.get('successCount', 0)
                            failure_count = asset_result.get('failureCount', 0)
                            logger.info(f"âœ… é™„ä»¶åˆ›å»ºç»“æœ: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failure_count} ä¸ª")
                            
                            # è®°å½•æˆåŠŸçš„é™„ä»¶è¯¦æƒ…ï¼ˆåŒ…å«æ•°æ®åº“IDï¼‰
                            if success_count > 0:
                                results = asset_result.get('results', [])
                                created_assets = []  # è®°å½•æˆåŠŸåˆ›å»ºçš„é™„ä»¶ä¿¡æ¯
                                for result_item in results:
                                    if result_item.get('success'):
                                        index = result_item.get('index', -1)
                                        asset_id = result_item.get('assetId')
                                        if index < len(assets_to_create):
                                            success_asset = assets_to_create[index]
                                            created_assets.append({
                                                'articleId': success_asset.get('articleId'),
                                                'fileName': success_asset.get('fileName'),
                                                'assetId': asset_id
                                            })
                                            logger.info(f"  âœ“ é™„ä»¶ [{index}] å·²å†™å…¥æ•°æ®åº“: ArticleId={success_asset.get('articleId')}, FileName={success_asset.get('fileName')}, AssetId={asset_id}")
                                
                                # éªŒè¯ï¼šå°è¯•æŸ¥è¯¢åˆšåˆ›å»ºçš„é™„ä»¶ï¼ˆå¯é€‰ï¼Œç”¨äºç¡®è®¤ï¼‰
                                if created_assets:
                                    logger.info(f"ğŸ“‹ é™„ä»¶è®°å½•å·²æˆåŠŸå†™å…¥æ•°æ®åº“ï¼Œå…± {len(created_assets)} æ¡è®°å½•")
                            
                            # è®°å½•å¤±è´¥çš„é™„ä»¶è¯¦æƒ…
                            if failure_count > 0:
                                results = asset_result.get('results', [])
                                for result_item in results:
                                    if not result_item.get('success'):
                                        index = result_item.get('index', -1)
                                        error = result_item.get('error', 'æœªçŸ¥é”™è¯¯')
                                        if index < len(assets_to_create):
                                            failed_asset = assets_to_create[index]
                                            logger.warning(f"  âœ— é™„ä»¶åˆ›å»ºå¤±è´¥ [{index}]: ArticleId={failed_asset.get('articleId')}, FileName={failed_asset.get('fileName')}, Error={error}")
                        else:
                            logger.error(f"âŒ åˆ›å»ºé™„ä»¶APIè°ƒç”¨å¤±è´¥: {asset_response.status_code} - {asset_response.text}")
                            logger.error(f"   å“åº”å†…å®¹: {asset_response.text[:500]}")
                    except Exception as e:
                        logger.error(f"åˆ›å»ºé™„ä»¶æ—¶å‡ºé”™: {str(e)}")
                        # é™„ä»¶åˆ›å»ºå¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            
            # åˆå¹¶å‰ç«¯è§£æå¤±è´¥å’Œåç«¯åˆ›å»ºå¤±è´¥
            total_failures = len(failures) + failure_count
            
            return ExcelImportResponse(
                total_rows=len(df),
                success_count=success_count,
                failure_count=total_failures,
                article_ids=article_ids,
                failures=failures
            )
    
    except pd.errors.EmptyDataError:
        logger.error("Excel æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
        raise HTTPException(status_code=400, detail="Excel æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
    except httpx.HTTPError as e:
        logger.error(f"è°ƒç”¨ .NET åç«¯å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=500,
            detail=f"è°ƒç”¨ .NET åç«¯å¤±è´¥: {str(e)}"
        )
    except Exception as e:
        logger.error(f"å¯¼å…¥å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=500,
            detail=f"å¯¼å…¥å¤±è´¥: {str(e)}"
        )


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "service": "ai-hub-ai"}


if __name__ == "__main__":
    import uvicorn
    import os
    port = int(os.getenv("PORT", "8000"))
    uvicorn.run(app, host="0.0.0.0", port=port)
