"""
AI Hub Excel å¯¼å…¥æœåŠ¡
FastAPI æœåŠ¡ï¼Œç”¨äºå¤„ç† Excel å¯¼å…¥å¹¶è°ƒç”¨ .NET åç«¯ API
"""
import os
import traceback
from typing import List, Optional
from io import BytesIO
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import pandas as pd
import httpx
from pathlib import Path
import json
import re
import logging
from urllib.parse import quote

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="AI Hub Excel å¯¼å…¥æœåŠ¡",
    description="å¤„ç† Excel æ–‡ä»¶å¯¼å…¥ï¼Œè½¬æ¢ä¸ºçŸ¥è¯†æ¡ç›®",
    version="1.0.0"
)

# é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
DOTNET_BASE_URL = os.getenv("DOTNET_BASE_URL", "http://localhost:5000")
INTERNAL_TOKEN = os.getenv("INTERNAL_TOKEN", "your-internal-token-change-in-production")
DEFAULT_TENANT = os.getenv("DEFAULT_TENANT", "default")
ATTACHMENT_BASE_PATH = os.getenv("ATTACHMENT_BASE_PATH", "")
ATTACHMENT_BASE_URL = os.getenv("ATTACHMENT_BASE_URL", "http://localhost:5000/uploads")

# åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from dotenv import load_dotenv
    load_dotenv()
    # é‡æ–°è¯»å–ç¯å¢ƒå˜é‡ï¼ˆ.env æ–‡ä»¶ä¸­çš„å€¼ä¼šè¦†ç›–ä¸Šé¢çš„é»˜è®¤å€¼ï¼‰
    DOTNET_BASE_URL = os.getenv("DOTNET_BASE_URL", DOTNET_BASE_URL)
    INTERNAL_TOKEN = os.getenv("INTERNAL_TOKEN", INTERNAL_TOKEN)
    DEFAULT_TENANT = os.getenv("DEFAULT_TENANT", DEFAULT_TENANT)
    ATTACHMENT_BASE_PATH = os.getenv("ATTACHMENT_BASE_PATH", ATTACHMENT_BASE_PATH)
    ATTACHMENT_BASE_URL = os.getenv("ATTACHMENT_BASE_URL", ATTACHMENT_BASE_URL)
    logger.info(f"é…ç½®åŠ è½½å®Œæˆ: DOTNET_BASE_URL={DOTNET_BASE_URL}, DEFAULT_TENANT={DEFAULT_TENANT}, ATTACHMENT_BASE_PATH={ATTACHMENT_BASE_PATH}")
except ImportError:
    logger.warning("python-dotenv æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼")


class ExcelImportResponse(BaseModel):
    """Excel å¯¼å…¥å“åº”"""
    total_rows: int
    success_count: int
    failure_count: int
    article_ids: List[int]
    failures: List[dict]


class ExcelRowFailure(BaseModel):
    """Excel è¡Œå¤„ç†å¤±è´¥ä¿¡æ¯"""
    row_index: int
    reason: str


def format_as_reasons(text: Optional[str]) -> Optional[str]:
    """æ ¼å¼åŒ–ä¸ºåŸå› åˆ—è¡¨ï¼ˆåŸå›  1ï¼š...ï¼‰"""
    if not text or not text.strip():
        return None
    
    # æŒ‰æ¢è¡Œç¬¦æˆ–åˆ†å·åˆ†å‰²
    reasons = [r.strip() for r in re.split(r'[\n\rï¼›;]', text) if r.strip()]
    
    if not reasons:
        return None
    
    # å¦‚æœå·²ç»æ˜¯"åŸå›  Xï¼š"æ ¼å¼ï¼Œç›´æ¥è¿”å›
    if any(re.match(r'^åŸå› \s*\d+[ï¼š:]', r) for r in reasons):
        return '\n'.join(reasons)
    
    # å¦åˆ™æ·»åŠ ç¼–å·
    return '\n'.join([f"åŸå›  {i + 1}ï¼š{r}" for i, r in enumerate(reasons)])


def format_as_steps(text: Optional[str]) -> Optional[str]:
    """æ ¼å¼åŒ–ä¸ºæ­¥éª¤åˆ—è¡¨ï¼ˆæ­¥éª¤ 1ï¼š...ï¼‰"""
    if not text or not text.strip():
        return None
    
    # æŒ‰æ¢è¡Œç¬¦æˆ–åˆ†å·åˆ†å‰²
    steps = [s.strip() for s in re.split(r'[\n\rï¼›;]', text) if s.strip()]
    
    if not steps:
        return None
    
    # å¦‚æœå·²ç»æ˜¯"æ­¥éª¤ Xï¼š"æ ¼å¼ï¼Œç›´æ¥è¿”å›
    if any(re.match(r'^æ­¥éª¤\s*\d+[ï¼š:]', s) for s in steps):
        return '\n'.join(steps)
    
    # å¦åˆ™æ·»åŠ ç¼–å·
    return '\n'.join([f"æ­¥éª¤ {i + 1}ï¼š{s}" for i, s in enumerate(steps)])


def extract_filename_from_reference(text: str) -> Optional[str]:
    """
    ä»æ–‡æœ¬å¼•ç”¨ä¸­æå–æ–‡ä»¶å
    æ”¯æŒæ ¼å¼ï¼š
    - å‚è€ƒ"xxx" æˆ– å‚è€ƒ"xxx"ï¼ˆä¸­æ–‡å¼•å· "" å’Œè‹±æ–‡å¼•å· ""ï¼‰
    - å‚è€ƒï¼šxxx
    - è§é™„ä»¶ï¼šxxx
    - å‚è€ƒ xxx
    """
    if not text or not text.strip():
        return None
    
    text = text.strip()
    
    # åŒ¹é…ï¼šå‚è€ƒ"xxx" æˆ– å‚è€ƒ"xxx"ï¼ˆæ”¯æŒä¸­æ–‡å¼•å· "" å’Œè‹±æ–‡å¼•å· ""ï¼‰
    # æ­£åˆ™ï¼šå‚è€ƒ[""]... [""]ï¼ˆæ”¯æŒä¸­æ–‡å¼•å·å’Œè‹±æ–‡å¼•å·ï¼‰
    # ä½¿ç”¨å­—ç¬¦ç±»åŒ¹é…æ‰€æœ‰ç±»å‹çš„å¼•å·
    match = re.search(r'å‚è€ƒ["""""]([^"""""]+)["""""]', text)
    if match:
        filename = match.group(1).strip()
        # å»é™¤å¯èƒ½æ®‹ç•™çš„å¼•å·ï¼ˆåŒ…æ‹¬å…¨è§’å•å¼•å· ''ï¼‰
        filename = filename.strip('"""\'""\'')
        return filename if filename else None
    
    # åŒ¹é…ï¼šå‚è€ƒï¼šxxx æˆ– å‚è€ƒ:xxx
    match = re.search(r'å‚è€ƒ[ï¼š:]\s*(.+)', text)
    if match:
        filename = match.group(1).strip()
        # å»é™¤å¯èƒ½çš„å‰åå¼•å·ï¼ˆåŒ…æ‹¬å…¨è§’å•å¼•å· ''ï¼‰
        filename = filename.strip('"""\'""\'')
        return filename if filename else None
    
    # åŒ¹é…ï¼šè§é™„ä»¶ï¼šxxx
    match = re.search(r'è§é™„ä»¶[ï¼š:]\s*(.+)', text)
    if match:
        filename = match.group(1).strip()
        filename = filename.strip('"""\'""\'')
        return filename if filename else None
    
    # åŒ¹é…ï¼šå‚è€ƒ xxxï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
    match = re.search(r'å‚è€ƒ\s+(.+)', text)
    if match:
        filename = match.group(1).strip()
        filename = filename.strip('"""\'""\'')
        return filename if filename else None
    
    # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œè¿”å›åŸæ–‡ï¼ˆå»é™¤"å‚è€ƒ"ç­‰å‰ç¼€ï¼‰
    filename = re.sub(r'^(å‚è€ƒ|è§é™„ä»¶)[ï¼š:\s]*', '', text)
    filename = filename.strip('"""\'""\'')
    return filename.strip() if filename.strip() else None


def _guess_asset_type_by_ext(ext: str) -> str:
    """æ ¹æ®æ‰©å±•ååˆ¤æ–­æ–‡ä»¶ç±»å‹"""
    ext = ext.lower()
    if ext in ['.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv']:
        return 'video'
    if ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']:
        return 'image'
    if ext in ['.pdf']:
        return 'pdf'
    if ext in ['.doc', '.docx', '.xls', '.xlsx', '.txt', '.ppt', '.pptx']:
        return 'other'
    return 'other'


def _should_skip_file(p: Path) -> bool:
    """è¿‡æ»¤ç³»ç»Ÿ/ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…è„æ•°æ®"""
    name = p.name.lower()
    if name in ['thumbs.db', '.ds_store', 'desktop.ini']:
        return True
    if name.startswith('~$'):  # office ä¸´æ—¶æ–‡ä»¶
        return True
    return False


def _build_file_info(base_path: Path, file_path: Path) -> dict:
    """æ„å»ºæ–‡ä»¶ä¿¡æ¯å­—å…¸"""
    file_size = file_path.stat().st_size
    relative_path = file_path.relative_to(base_path)
    relative_path_str = str(relative_path).replace(os.sep, '/')
    encoded_path = '/'.join(quote(part, safe='') for part in relative_path_str.split('/'))
    file_url = f"{ATTACHMENT_BASE_URL.rstrip('/')}/{encoded_path}"
    return {
        "path": str(file_path),
        "url": file_url,
        "type": _guess_asset_type_by_ext(file_path.suffix),
        "size": file_size,
        "file_name": file_path.name,
        "relative_path": relative_path_str,
    }


def find_attachment_files(filename: str) -> List[dict]:
    """
    åœ¨å›ºå®šç›®å½•ä¸­é€’å½’æŸ¥æ‰¾é™„ä»¶æ–‡ä»¶ï¼ˆæ”¯æŒæ–‡ä»¶å¤¹åµŒå¥—ï¼‰
    è¿”å›ï¼šList[{ "path","url","type","size","file_name", "relative_path" }]
    - å‘½ä¸­æ–‡ä»¶ï¼šè¿”å› 1 æ¡
    - å‘½ä¸­æ–‡ä»¶å¤¹ï¼šè¿”å›è¯¥æ–‡ä»¶å¤¹å†…æ‰€æœ‰æ–‡ä»¶ï¼ˆé€’å½’ï¼‰çš„å¤šæ¡
    - æœªå‘½ä¸­ï¼šè¿”å›ç©ºåˆ—è¡¨
    """
    if not ATTACHMENT_BASE_PATH or not ATTACHMENT_BASE_PATH.strip():
        logger.debug(f"ATTACHMENT_BASE_PATH æœªé…ç½®ï¼Œè·³è¿‡æ–‡ä»¶æŸ¥æ‰¾: {filename}")
        return []

    base_path = Path(ATTACHMENT_BASE_PATH.strip())
    if not base_path.exists():
        logger.debug(f"é™„ä»¶åŸºç¡€è·¯å¾„ä¸å­˜åœ¨: {base_path}ï¼Œè·³è¿‡æ–‡ä»¶æŸ¥æ‰¾: {filename}")
        return []

    # æ¸…ç†æ–‡ä»¶åï¼šå»é™¤å¼•å·ã€æ‹¬å·ç­‰åŒ…è£¹ç¬¦å·ï¼ˆåŒ…æ‹¬å…¨è§’å•å¼•å· ''ï¼‰
    clean_filename = filename.strip().strip('"""\'""\'ã€Šã€‹ã€ã€‘[]()ï¼ˆï¼‰').strip()

    # å¦‚æœç”¨æˆ·å†™äº†æ‰©å±•åï¼Œå…ˆè½¬ stemï¼ˆExcel é‡Œå¯èƒ½ä¸å¸¦æ‰©å±•åï¼‰
    if '.' in clean_filename:
        clean_filename = Path(clean_filename).stem

    # æ‰©å±•åæ˜ å°„ï¼ˆç”¨äº"æ–‡ä»¶å‘½ä¸­"é˜¶æ®µï¼‰
    extensions_map = {
        'video': ['.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv'],
        'image': ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'],
        'pdf': ['.pdf'],
        'other': ['.doc', '.docx', '.xls', '.xlsx', '.txt', '.ppt', '.pptx']
    }

    # ========== 1) ç²¾ç¡®åŒ¹é… / é€’å½’ç²¾ç¡®åŒ¹é…ï¼ˆå‘½ä¸­æ–‡ä»¶å°±è¿”å› 1 æ¡ï¼‰ ==========
    for asset_type, extensions in extensions_map.items():
        for ext in extensions:
            full_filename = clean_filename + ext

            # æ ¹ç›®å½•ç²¾ç¡®åŒ¹é…
            root_file = base_path / full_filename
            if root_file.is_file():
                logger.info(f"æ‰¾åˆ°é™„ä»¶æ–‡ä»¶ï¼ˆæ ¹ç›®å½•ç²¾ç¡®åŒ¹é…ï¼‰: {clean_filename} -> {root_file.name}")
                info = _build_file_info(base_path, root_file)
                info["type"] = asset_type  # æŒ‰æ˜ å°„å¼ºåˆ¶ç±»å‹
                return [info]

            # å…¨ç›®å½•ç²¾ç¡®åŒ¹é…
            for file_path in base_path.rglob(full_filename):
                if file_path.is_file():
                    logger.info(f"æ‰¾åˆ°é™„ä»¶æ–‡ä»¶ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰: {clean_filename} -> {file_path.name}")
                    info = _build_file_info(base_path, file_path)
                    info["type"] = asset_type
                    return [info]

            # ========== 2) æ¨¡ç³ŠåŒ¹é…ï¼ˆå‘½ä¸­æ–‡ä»¶å°±è¿”å› 1 æ¡ï¼‰ ==========
            pattern = f"*{clean_filename}*{ext}"

            root_matches = list(base_path.glob(pattern))
            matches = root_matches + list(base_path.rglob(pattern))

            # å»é‡
            seen = set()
            unique_matches = []
            for m in matches:
                if m not in seen:
                    seen.add(m)
                    unique_matches.append(m)
            matches = unique_matches

            if matches:
                best_match = None
                clean_lower = clean_filename.lower()

                for p in matches:
                    if p.is_file() and clean_lower in p.stem.lower():
                        best_match = p
                        break

                if not best_match:
                    for p in matches:
                        if p.is_file():
                            best_match = p
                            break

                if best_match and best_match.is_file():
                    logger.info(f"æ‰¾åˆ°é™„ä»¶æ–‡ä»¶ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰: {clean_filename} -> {best_match.name}")
                    info = _build_file_info(base_path, best_match)
                    info["type"] = asset_type
                    return [info]

    # ========== 3) æ–‡ä»¶å¤¹å‘½ä¸­ï¼šè¿”å›è¯¥æ–‡ä»¶å¤¹å†…æ‰€æœ‰æ–‡ä»¶ï¼ˆé€’å½’ï¼Œå¤šæ¡ï¼‰ ==========
    logger.debug(f"æ–‡ä»¶åŒ¹é…å¤±è´¥ï¼Œå°è¯•åŒ¹é…æ–‡ä»¶å¤¹åç§°: {clean_filename}")
    clean_lower = clean_filename.lower()

    matched_folders = []
    for folder_path in base_path.rglob("*"):
        if not folder_path.is_dir():
            continue
        folder_lower = folder_path.name.lower()

        # å®Œå…¨åŒ¹é…ä¼˜å…ˆ
        if clean_lower == folder_lower:
            matched_folders.insert(0, (folder_path, 1))
        # åŒ…å«åŒ¹é…å…œåº•
        elif clean_lower in folder_lower or folder_lower in clean_lower:
            matched_folders.append((folder_path, 2))

    matched_folders.sort(key=lambda x: x[1])

    for folder_path, priority in matched_folders:
        # é€’å½’æ”¶é›†æ–‡ä»¶å¤¹å†…æ‰€æœ‰æ–‡ä»¶
        all_files: List[Path] = []
        for p in folder_path.rglob("*"):
            if p.is_file() and not _should_skip_file(p):
                all_files.append(p)

        if not all_files:
            continue

        # æ’åºä¿è¯ç¨³å®šï¼ˆæ–¹ä¾¿æ¯”å¯¹å¯¼å…¥ç»“æœï¼‰
        all_files.sort(key=lambda p: str(p).lower())

        results = []
        for p in all_files:
            info = _build_file_info(base_path, p)
            results.append(info)

        match_type = "å®Œå…¨åŒ¹é…" if priority == 1 else "åŒ…å«åŒ¹é…"
        logger.info(
            f"æ‰¾åˆ°é™„ä»¶æ–‡ä»¶ï¼ˆæ–‡ä»¶å¤¹{match_type}ï¼‰: {clean_filename} -> æ–‡ä»¶å¤¹[{folder_path.name}] å…± {len(results)} ä¸ªæ–‡ä»¶"
        )
        return results

    # P1: æœªå‘½ä¸­æ—¶æ‰“å°"æ ¹ç›®å½•/å…¨ç›®å½•"å€™é€‰ stemï¼ˆé™åˆ¶ 5 ä¸ªï¼‰
    logger.warning(f"æœªæ‰¾åˆ°é™„ä»¶æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹: {filename} (æ¸…ç†å: {clean_filename}, æœç´¢è·¯å¾„: {base_path})")
    
    # åˆ—å‡ºæ ¹ç›®å½•ä¸‹å¯èƒ½çš„å€™é€‰æ–‡ä»¶ï¼ˆé™åˆ¶ 5 ä¸ªï¼‰
    try:
        root_candidates = []
        for file_path in base_path.iterdir():
            if file_path.is_file() and clean_filename.lower() in file_path.stem.lower():
                root_candidates.append(file_path.stem)
                if len(root_candidates) >= 5:
                    break
        if root_candidates:
            logger.debug(f"  æ ¹ç›®å½•å€™é€‰æ–‡ä»¶ï¼ˆstemï¼‰: {root_candidates}")
    except Exception as e:
        logger.debug(f"  æ— æ³•åˆ—å‡ºæ ¹ç›®å½•å€™é€‰æ–‡ä»¶: {str(e)}")
    
    # åˆ—å‡ºå…¨ç›®å½•ä¸‹å¯èƒ½çš„å€™é€‰æ–‡ä»¶ï¼ˆé™åˆ¶ 5 ä¸ªï¼‰
    try:
        all_candidates = []
        for file_path in base_path.rglob("*"):
            if file_path.is_file() and clean_filename.lower() in file_path.stem.lower():
                all_candidates.append(file_path.stem)
                if len(all_candidates) >= 5:
                    break
        if all_candidates and len(all_candidates) > len(root_candidates) if 'root_candidates' in locals() else True:
            logger.debug(f"  å…¨ç›®å½•å€™é€‰æ–‡ä»¶ï¼ˆstemï¼‰: {all_candidates}")
    except Exception as e:
        logger.debug(f"  æ— æ³•åˆ—å‡ºå…¨ç›®å½•å€™é€‰æ–‡ä»¶: {str(e)}")
    
    return []


def map_excel_row_to_article(row: pd.Series, source_file_name: str, sheet_name: str, row_index: int) -> Optional[dict]:
    """
    å°† Excel è¡Œæ˜ å°„ä¸ºçŸ¥è¯†æ¡ç›® DTOï¼ˆå¿ å®è¿˜åŸåŸæ–‡æ¡£æ¨¡å¼ï¼‰
    
    è¡¨å¤´ï¼šåºå· | ç°è±¡ï¼ˆé—®é¢˜ï¼‰ | æ£€æŸ¥ç‚¹ï¼ˆåŸå› ï¼‰ | ç»´ä¿®å¯¹ç­–ï¼ˆè§£å†³åŠæ³•ï¼‰ | ç»´ä¿®è§†é¢‘ï¼ˆé™„ä»¶ï¼‰
    """
    # è¯»å–å­—æ®µï¼ˆåŸæ ·ä¿ç•™ï¼Œä¸ä¿®æ”¹ï¼‰
    # æ³¨æ„ï¼špandas è¯»å–æ—¶ï¼Œåˆ—åå¯èƒ½åŒ…å«å‰åç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦
    # æ”¯æŒå¤šç§åˆ—åæ ¼å¼ï¼ˆå¸¦æ‹¬å·å’Œä¸å¸¦æ‹¬å·ï¼‰
    serial_number = ""
    for col_name in ["åºå·"]:
        if col_name in row.index and pd.notna(row.get(col_name)):
            serial_number = str(row.get(col_name)).strip()
            break
    
    phenomenon = ""
    # å°è¯•å¤šç§å¯èƒ½çš„åˆ—åï¼ˆåŒ…æ‹¬å¸¦æ‹¬å·å’Œä¸å¸¦æ‹¬å·çš„å˜ä½“ï¼‰
    for col_name in ["ç°è±¡ï¼ˆé—®é¢˜ï¼‰", "ç°è±¡(é—®é¢˜)", "ç°è±¡ ï¼ˆé—®é¢˜ï¼‰", "ç°è±¡ (é—®é¢˜)", "ç°è±¡", "é—®é¢˜"]:
        if col_name in row.index and pd.notna(row.get(col_name)):
            val = row.get(col_name)
            if val is not None and str(val).strip():
                phenomenon = str(val).strip()
                break
    
    checkpoints = ""
    for col_name in ["æ£€æŸ¥ç‚¹ï¼ˆåŸå› ï¼‰", "æ£€æŸ¥ç‚¹(åŸå› )", "æ£€æŸ¥ç‚¹ ï¼ˆåŸå› ï¼‰", "æ£€æŸ¥ç‚¹ (åŸå› )", "æ£€æŸ¥ç‚¹", "åŸå› "]:
        if col_name in row.index and pd.notna(row.get(col_name)):
            val = row.get(col_name)
            if val is not None and str(val).strip():
                checkpoints = str(val).strip()
                break
    
    solution = ""
    for col_name in ["ç»´ä¿®å¯¹ç­–ï¼ˆè§£å†³åŠæ³•ï¼‰", "ç»´ä¿®å¯¹ç­–(è§£å†³åŠæ³•)", "ç»´ä¿®å¯¹ç­– ï¼ˆè§£å†³åŠæ³•ï¼‰", "ç»´ä¿®å¯¹ç­– (è§£å†³åŠæ³•)", "å¯¹ç­–", "ç»´ä¿®å¯¹ç­–", "è§£å†³åŠæ³•", "è§£å†³æ–¹æ³•"]:
        if col_name in row.index and pd.notna(row.get(col_name)):
            val = row.get(col_name)
            if val is not None and str(val).strip():
                solution = str(val).strip()
                break
    
    video_reference = ""
    for col_name in ["ç»´ä¿®è§†é¢‘ï¼ˆé™„ä»¶ï¼‰", "ç»´ä¿®è§†é¢‘(é™„ä»¶)", "ç»´ä¿®è§†é¢‘ ï¼ˆé™„ä»¶ï¼‰", "ç»´ä¿®è§†é¢‘ (é™„ä»¶)", "ç»´ä¿®è§†é¢‘", "é™„ä»¶", "å¤‡æ³¨"]:
        if col_name in row.index and pd.notna(row.get(col_name)):
            val = row.get(col_name)
            if val is not None and str(val).strip():
                video_reference = str(val).strip()
                break
    
    # è·³è¿‡ç©ºè¡Œï¼ˆå¿…é¡»æœ‰ç°è±¡ï¼ˆé—®é¢˜ï¼‰ï¼‰
    if not phenomenon:
        return None
    
    # 1) title = ã€YH400/YH500ã€‘ + ç°è±¡ï¼ˆé—®é¢˜ï¼‰
    title = f"ã€YH400/YH500ã€‘{phenomenon}".strip()
    if not title:
        return None
    
    # 2) question_text å¿…é¡»ä¿ç•™åŸå­—æ®µåä¸åŸæ–‡
    question_parts = []
    if serial_number:
        question_parts.append(f"ã€åºå·ã€‘{serial_number}")
    if phenomenon:
        question_parts.append(f"ã€ç°è±¡ï¼ˆé—®é¢˜ï¼‰ã€‘{phenomenon}")
    question_text = "\n".join(question_parts) if question_parts else None
    
    # 3) cause_text = åŸæ ·å†™å…¥ æ£€æŸ¥ç‚¹ï¼ˆåŸå› ï¼‰ï¼ˆä¿ç•™ç¼–å·/æ¢è¡Œï¼‰
    cause_text = None
    if checkpoints:
        cause_text = f"ã€æ£€æŸ¥ç‚¹ï¼ˆåŸå› ï¼‰ã€‘\n{checkpoints}"
    
    # 4) solution_text = åŸæ ·å†™å…¥ ç»´ä¿®å¯¹ç­–ï¼ˆè§£å†³åŠæ³•ï¼‰ï¼ˆä¿ç•™ç¼–å·/æ¢è¡Œï¼‰
    solution_parts = []
    if solution:
        solution_parts.append(f"ã€ç»´ä¿®å¯¹ç­–ï¼ˆè§£å†³åŠæ³•ï¼‰ã€‘\n{solution}")
    
    # 5) ç»´ä¿®è§†é¢‘ï¼ˆé™„ä»¶ï¼‰ï¼šå¦‚æœæ˜¯æ–‡æœ¬ï¼Œè¿½åŠ åˆ° solution_text
    if video_reference:
        solution_parts.append(f"ã€ç»´ä¿®è§†é¢‘ï¼ˆé™„ä»¶ï¼‰ã€‘\n{video_reference}")
    
    solution_text = "\n\n".join(solution_parts) if solution_parts else None
    
    # 6) scope_json å¿…é¡»åŒ…å«ï¼šè®¾å¤‡ç³»åˆ—ã€æ¥æºæ–‡ä»¶ã€sheetã€è¡Œå·
    scope_data = {
        "è®¾å¤‡ç³»åˆ—": "YH400/YH500",
        "æ¥æºæ–‡ä»¶": source_file_name,
        "sheet": sheet_name,
        "è¡Œå·": row_index
    }
    scope_json = json.dumps(scope_data, ensure_ascii=False)
    
    # 7) tags è‡³å°‘åŒ…å«ï¼šYH400, YH500, æ¥æº:{æ–‡ä»¶å}
    tags = ["YH400", "YH500"]
    file_name_without_ext = Path(source_file_name).stem
    if file_name_without_ext:
        tags.append(f"æ¥æº:{file_name_without_ext}")
    
    # å¦‚æœç°è±¡/åŸå› ä¸­åŒ…å«å…³é”®å­—ä¹Ÿå¯è¿½åŠ ï¼ˆç®€å•å…³é”®è¯æå–ï¼‰
    text_content = f"{phenomenon} {checkpoints} {solution}".lower()
    if "yh400" in text_content and "YH400" not in tags:
        # å·²ç»åœ¨tagsä¸­ï¼Œè·³è¿‡
        pass
    if "yh500" in text_content and "YH500" not in tags:
        # å·²ç»åœ¨tagsä¸­ï¼Œè·³è¿‡
        pass
    
    # æå–é™„ä»¶ä¿¡æ¯ï¼ˆç”¨äºåç»­åˆ›å»º kb_assetï¼‰
    # æ”¯æŒä¸€ä¸ªå•å…ƒæ ¼åŒ…å«å¤šä¸ªå¼•ç”¨ï¼ˆç”¨æ¢è¡Œç¬¦ã€åˆ†å·ç­‰åˆ†éš”ï¼‰ï¼Œæ¯ä¸ªå¼•ç”¨éƒ½å°è¯•åŒ¹é…å¹¶åˆ›å»ºé™„ä»¶è®°å½•
    attachment_info_list = []  # æ”¹ä¸ºåˆ—è¡¨ï¼Œæ”¯æŒå¤šä¸ªé™„ä»¶
    has_attachment_reference = False  # æ ‡è®°æ˜¯å¦æœ‰é™„ä»¶å¼•ç”¨ï¼ˆæ— è®ºæ˜¯å¦æ‰¾åˆ°æ–‡ä»¶ï¼‰
    if video_reference:
        has_attachment_reference = True
        # å…ˆæŒ‰æ¢è¡Œç¬¦åˆ†å‰²ï¼Œç„¶åå¯¹æ¯ä¸€è¡Œå†æŒ‰åˆ†å·ã€ä¸­æ–‡åˆ†å·ç­‰åˆ†å‰²
        # è¿™æ ·å¯ä»¥å¤„ç†å¤šç§æ ¼å¼ï¼šæ¢è¡Œåˆ†éš”ã€åˆ†å·åˆ†éš”ã€æˆ–æ··åˆ
        all_references = []
        for line in video_reference.split('\n'):
            line = line.strip()
            if not line:
                continue
            # æŒ‰åˆ†å·ã€ä¸­æ–‡åˆ†å·åˆ†å‰²
            parts = re.split(r'[ï¼›;]', line)
            for part in parts:
                part = part.strip()
                if part:
                    all_references.append(part)
        
        # P0: å¦‚æœä¸€æ®µé‡Œå‡ºç°å¤šä¸ª"å‚è€ƒ"ï¼Œå¼ºåˆ¶äºŒæ¬¡æ­£åˆ™æå–å¹¶å±•å¼€
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ª"å‚è€ƒ"å…³é”®å­—
        reference_count = video_reference.count('å‚è€ƒ')
        if reference_count > 1 or len(all_references) == 1:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰"å‚è€ƒ"xxx""æ ¼å¼çš„å¼•ç”¨ï¼ˆæ”¯æŒä¸­æ–‡å¼•å· "" å’Œè‹±æ–‡å¼•å· ""ï¼‰
            # æ­£åˆ™ï¼šå‚è€ƒ[""]... [""]ï¼ˆæ”¯æŒä¸­æ–‡å¼•å·å’Œè‹±æ–‡å¼•å·ï¼‰
            pattern = r'å‚è€ƒ["""""]([^"""""]+)["""""]'
            matches = re.findall(pattern, video_reference)
            if matches:
                # å¦‚æœæ­£åˆ™æå–åˆ°æ›´å¤šå¼•ç”¨ï¼Œä½¿ç”¨æ­£åˆ™æå–çš„ç»“æœ
                if len(matches) > len(all_references):
                    all_references = [f'å‚è€ƒ"{m}"' for m in matches]
                    logger.debug(f"é€šè¿‡æ­£åˆ™æå–åˆ° {len(all_references)} ä¸ªå¼•ç”¨ï¼ˆåŸåˆ†å‰²ç»“æœ: {len(all_references) - len(matches) + len(matches)} ä¸ªï¼‰")
                elif len(matches) == len(all_references) and reference_count > 1:
                    # å³ä½¿æ•°é‡ç›¸åŒï¼Œå¦‚æœæ£€æµ‹åˆ°å¤šä¸ª"å‚è€ƒ"ï¼Œä¹Ÿä½¿ç”¨æ­£åˆ™æå–çš„ç»“æœï¼ˆæ›´å‡†ç¡®ï¼‰
                    all_references = [f'å‚è€ƒ"{m}"' for m in matches]
                    logger.debug(f"æ£€æµ‹åˆ°å¤šä¸ª'å‚è€ƒ'ï¼Œä½¿ç”¨æ­£åˆ™æå–ç»“æœ: {len(all_references)} ä¸ªå¼•ç”¨")
        
        if len(all_references) > 1:
            logger.info(f"æ£€æµ‹åˆ° {len(all_references)} ä¸ªå¼•ç”¨ï¼Œå°†å°è¯•åŒ¹é…æ‰€æœ‰å¼•ç”¨")
        else:
            logger.debug(f"æå–åˆ° {len(all_references)} ä¸ªå¼•ç”¨: {all_references}")
        
        matched_count = 0
        for idx, ref_line in enumerate(all_references, 1):
            extracted_filename = extract_filename_from_reference(ref_line)
            if extracted_filename:
                # å†æ¬¡æ¸…ç†æ–‡ä»¶åï¼ˆç¡®ä¿å»é™¤æ‰€æœ‰å¼•å·ï¼ŒåŒ…æ‹¬å…¨è§’å•å¼•å· ''ï¼‰
                clean_extracted = extracted_filename.strip('"""\'""\'ã€Šã€‹ã€ã€‘[]()ï¼ˆï¼‰').strip()
                if not clean_extracted:
                    logger.debug(f"å¼•ç”¨ {idx}/{len(all_references)}: æ¸…ç†åæ–‡ä»¶åä¸ºç©º (åŸå§‹: {ref_line[:50]}...)")
                    continue
                
                file_infos = find_attachment_files(clean_extracted)
                if file_infos:
                    # æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯å•ä¸ªæ–‡ä»¶æˆ–å¤šä¸ªæ–‡ä»¶ï¼‰ï¼Œå…¨éƒ¨æ·»åŠ åˆ°åˆ—è¡¨
                    for file_info in file_infos:
                        attachment_info_list.append({
                            "filename": clean_extracted,
                            "file_name": file_info.get("file_name", os.path.basename(file_info["path"])),
                            "url": file_info["url"],
                            "asset_type": file_info["type"],
                            "size": file_info["size"],
                            "relative_path": file_info.get("relative_path"),
                            "source_ref": clean_extracted,  # Excel å¼•ç”¨åï¼ˆç”¨äºæº¯æºï¼‰
                            "source_folder": Path(file_info["path"]).parent.name if file_info.get("path") else None
                        })
                    matched_count += len(file_infos)
                    if len(file_infos) == 1:
                        logger.info(f"âœ“ [{idx}/{len(all_references)}] æ‰¾åˆ°é™„ä»¶: {clean_extracted} -> {file_infos[0]['url']}")
                    else:
                        logger.info(f"âœ“ [{idx}/{len(all_references)}] æ‰¾åˆ°é™„ä»¶ï¼ˆæ–‡ä»¶å¤¹ï¼‰: {clean_extracted} -> {len(file_infos)} ä¸ªæ–‡ä»¶")
                else:
                    logger.warning(f"âœ— [{idx}/{len(all_references)}] æœªæ‰¾åˆ°é™„ä»¶: {clean_extracted} (åŸå§‹å¼•ç”¨: {ref_line[:50]}...)")
            else:
                logger.debug(f"å¼•ç”¨ {idx}/{len(all_references)}: æ— æ³•æå–æ–‡ä»¶å (åŸå§‹: {ref_line[:50]}...)")
        
        if len(all_references) > 1:
            # P1: åŒ¹é…ç»Ÿè®¡æ‹†æˆ "å‘½ä¸­å¼•ç”¨æ•° / æ€»å¼•ç”¨æ•° + æ–‡ä»¶æ€»æ•°"
            total_files = len(attachment_info_list)
            logger.info(f"å¤šå¼•ç”¨åŒ¹é…ç»“æœ: {matched_count}/{len(all_references)} ä¸ªå¼•ç”¨æ‰¾åˆ°æ–‡ä»¶ï¼Œå…± {total_files} ä¸ªæ–‡ä»¶")
    
    # _attachment_info å§‹ç»ˆæ˜¯åˆ—è¡¨ï¼ˆç»Ÿä¸€å¤„ç†ï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜ï¼‰
    attachment_info = attachment_info_list if attachment_info_list else None
    
    # å»é‡ï¼šåŸºäº URL æˆ– relative_path å»é‡ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
    if attachment_info:
        seen_urls = set()
        unique_attachments = []
        for att in attachment_info:
            url_key = att.get("url") or att.get("relative_path")
            if url_key and url_key not in seen_urls:
                seen_urls.add(url_key)
                unique_attachments.append(att)
        attachment_info = unique_attachments if unique_attachments else None
    
    return {
        "title": title,
        "questionText": question_text,
        "causeText": cause_text,
        "solutionText": solution_text,
        "scopeJson": scope_json,
        "tags": ", ".join(tags),
        "createdBy": "ç³»ç»Ÿå¯¼å…¥",
        "_attachment_info": attachment_info,  # å†…éƒ¨å­—æ®µï¼Œå§‹ç»ˆæ˜¯åˆ—è¡¨æˆ– Noneï¼Œä¸å‘é€ç»™åç«¯
        "_has_attachment_reference": has_attachment_reference  # æ ‡è®°æ˜¯å¦æœ‰é™„ä»¶å¼•ç”¨ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
    }


@app.post("/import/excel", response_model=ExcelImportResponse)
async def import_excel(file: UploadFile = File(...)):
    """
    å¯¼å…¥ Excel æ–‡ä»¶ä¸ºçŸ¥è¯†æ¡ç›®
    
    - æ¥æ”¶ .xlsx æ–‡ä»¶
    - ä½¿ç”¨ pandas è¯»å–
    - æ¯è¡Œæ˜ å°„ä¸ºä¸€æ¡çŸ¥è¯†è‰ç¨¿
    - è°ƒç”¨ .NET çš„ /api/ai/kb/articles/batch åˆ›å»ºè‰ç¨¿
    - å¦‚æœæ‰¾åˆ°é™„ä»¶æ–‡ä»¶ï¼Œåˆ›å»º kb_asset è®°å½•ï¼ˆæ–¹æ¡ˆ Bï¼šå…ƒæ•°æ®å…³è”ï¼‰
    """
    logger.info(f"æ”¶åˆ° Excel å¯¼å…¥è¯·æ±‚: {file.filename}")
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    if not file.filename or not file.filename.endswith('.xlsx'):
        logger.warning(f"æ–‡ä»¶ç±»å‹ä¸æ­£ç¡®: {file.filename}")
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ .xlsx æ ¼å¼çš„ Excel æ–‡ä»¶")
    
    try:
        # è¯»å– Excel æ–‡ä»¶
        contents = await file.read()
        # å°† bytes è½¬æ¢ä¸º BytesIO å¯¹è±¡ï¼Œpandas æ‰èƒ½è¯»å–
        excel_file = BytesIO(contents)
        
        # å…ˆå°è¯•ä»ç¬¬ä¸€è¡Œè¯»å–è¡¨å¤´
        df = pd.read_excel(excel_file, engine='openpyxl', header=0)
        
        # éªŒè¯å¿…éœ€å­—æ®µï¼ˆæ”¯æŒå¤šç§åˆ—åï¼‰
        # æ•…éšœç°è±¡åˆ—åå¯èƒ½ä¸ºï¼šæ•…éšœç°è±¡ã€ç°è±¡ï¼ˆé—®é¢˜ï¼‰ã€ç°è±¡ã€é—®é¢˜
        fault_phenomenon_columns = ["æ•…éšœç°è±¡", "ç°è±¡ï¼ˆé—®é¢˜ï¼‰", "ç°è±¡(é—®é¢˜)", "ç°è±¡", "é—®é¢˜"]
        has_fault_phenomenon = any(col in df.columns for col in fault_phenomenon_columns)
        header_row = 0  # è®°å½• header è¡Œå·
        
        # å¦‚æœç¬¬ä¸€è¡Œæ²¡æœ‰æ‰¾åˆ°å¿…éœ€å­—æ®µï¼Œå°è¯•ä»ç¬¬äºŒè¡Œè¯»å–ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰
        if not has_fault_phenomenon:
            logger.info("ç¬¬ä¸€è¡Œæœªæ‰¾åˆ°å¿…éœ€å­—æ®µï¼Œå°è¯•ä»ç¬¬äºŒè¡Œè¯»å–è¡¨å¤´ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰")
            excel_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            df = pd.read_excel(excel_file, engine='openpyxl', header=1)
            header_row = 1  # æ›´æ–° header è¡Œå·
            
            # å†æ¬¡éªŒè¯
            has_fault_phenomenon = any(col in df.columns for col in fault_phenomenon_columns)
            if not has_fault_phenomenon:
                raise HTTPException(
                    status_code=400,
                    detail=f"ç¼ºå°‘å¿…éœ€å­—æ®µ: è¯·åŒ…å«ä»¥ä¸‹ä»»ä¸€åˆ—å - {', '.join(fault_phenomenon_columns)}"
                )
        
        if df.empty:
            raise HTTPException(status_code=400, detail="Excel æ–‡ä»¶ä¸ºç©º")
        
        logger.info(f"è¯»å–åˆ° {len(df)} è¡Œæ•°æ®")
        logger.info(f"åˆ—å: {list(df.columns)}")
        
        # è·å– sheet åç§°ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        sheet_name = "Sheet1"  # é»˜è®¤å€¼
        try:
            # å°è¯•ä» Excel æ–‡ä»¶ä¸­è·å– sheet åç§°
            excel_file.seek(0)
            import openpyxl
            wb = openpyxl.load_workbook(excel_file, read_only=True)
            if wb.sheetnames:
                sheet_name = wb.sheetnames[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ª sheet çš„åç§°
            excel_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        except Exception as e:
            logger.warning(f"æ— æ³•è·å– sheet åç§°ï¼Œä½¿ç”¨é»˜è®¤å€¼: {str(e)}")
        
        # ç¡®å®š header è¡Œå·ï¼ˆç”¨äºè®¡ç®— Excel è¡Œå·ï¼‰
        header_row = 0 if has_fault_phenomenon else 1
        
        # æ˜ å°„æ¯è¡Œä¸ºçŸ¥è¯†æ¡ç›® DTO
        articles = []  # å‘é€ç»™åç«¯çš„æ–‡ç« ï¼ˆä¸åŒ…å«å†…éƒ¨å­—æ®µï¼‰
        articles_with_attachments = []  # åŒ…å«é™„ä»¶ä¿¡æ¯çš„å®Œæ•´æ–‡ç« æ•°æ®
        failures = []
        skipped_rows = 0
        attachment_match_count = 0  # é™„ä»¶åŒ¹é…ç»Ÿè®¡
        attachment_not_found_count = 0  # é™„ä»¶æœªæ‰¾åˆ°ç»Ÿè®¡
        
        for idx, row in df.iterrows():
            try:
                # Excel è¡Œå· = pandas index + headerè¡Œæ•° + 1ï¼ˆExcelä»1å¼€å§‹è®¡æ•°ï¼‰
                # å¦‚æœ header=0ï¼Œåˆ™ Excelè¡Œå· = idx + 2ï¼ˆç¬¬1è¡Œæ˜¯è¡¨å¤´ï¼Œç¬¬2è¡Œå¼€å§‹æ˜¯æ•°æ®ï¼‰
                # å¦‚æœ header=1ï¼Œåˆ™ Excelè¡Œå· = idx + 3ï¼ˆç¬¬1è¡Œæ˜¯æ ‡é¢˜ï¼Œç¬¬2è¡Œæ˜¯è¡¨å¤´ï¼Œç¬¬3è¡Œå¼€å§‹æ˜¯æ•°æ®ï¼‰
                excel_row_number = int(idx) + header_row + 2
                
                # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€è¡Œçš„æ•°æ®
                if idx == 0:
                    logger.info(f"ç¬¬ä¸€è¡Œæ•°æ®é¢„è§ˆ: {dict(row)}")
                
                article = map_excel_row_to_article(row, file.filename, sheet_name, excel_row_number)
                if article:
                    # ä¿å­˜åŒ…å«é™„ä»¶ä¿¡æ¯çš„åŸå§‹ article
                    articles_with_attachments.append(article)
                    # ç§»é™¤å†…éƒ¨å­—æ®µï¼ˆä¸å‘é€ç»™åç«¯ï¼‰
                    article_for_api = {k: v for k, v in article.items() if not k.startswith("_")}
                    articles.append(article_for_api)
                else:
                    skipped_rows += 1
                    if idx < 3:  # åªè®°å½•å‰3è¡Œçš„è·³è¿‡åŸå› 
                        logger.info(f"ç¬¬ {excel_row_number} è¡Œè¢«è·³è¿‡ï¼ˆç©ºè¡Œæˆ–ç¼ºå°‘å¿…éœ€å­—æ®µï¼‰")
                # å¦‚æœè¿”å› Noneï¼Œè¯´æ˜æ˜¯ç©ºè¡Œï¼Œè·³è¿‡
            except Exception as e:
                excel_row_number = int(idx) + header_row + 2
                logger.error(f"å¤„ç†ç¬¬ {excel_row_number} è¡Œæ—¶å‡ºé”™: {str(e)}")
                failures.append({
                    "row_index": excel_row_number,
                    "reason": str(e)
                })
        
        # ç»Ÿè®¡é™„ä»¶åŒ¹é…æƒ…å†µï¼ˆæ”¯æŒå¤šä¸ªé™„ä»¶ï¼‰
        attachment_match_count = 0
        for a in articles_with_attachments:
            att_info = a.get("_attachment_info")
            if att_info:
                # _attachment_info ç°åœ¨å§‹ç»ˆæ˜¯åˆ—è¡¨
                if isinstance(att_info, list):
                    attachment_match_count += len(att_info)
                else:
                    # å…¼å®¹æ—§æ•°æ®
                    attachment_match_count += 1
        
        attachment_total = sum(1 for a in articles_with_attachments if a.get("_has_attachment_reference", False))
        attachment_not_found_count = attachment_total - sum(1 for a in articles_with_attachments if a.get("_attachment_info"))
        
        logger.info(f"æˆåŠŸæ˜ å°„ {len(articles)} æ¡ï¼Œè·³è¿‡ {skipped_rows} è¡Œï¼Œå¤±è´¥ {len(failures)} è¡Œ")
        if attachment_total > 0:
            match_rate = (attachment_match_count / attachment_total * 100) if attachment_total > 0 else 0
            logger.info(f"ğŸ“ é™„ä»¶åŒ¹é…ç»Ÿè®¡: æ‰¾åˆ° {attachment_match_count}/{attachment_total} ä¸ª ({match_rate:.1f}%)ï¼Œæœªæ‰¾åˆ° {attachment_not_found_count} ä¸ª")
        
        if not articles:
            error_msg = f"æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®è¡Œã€‚æ€»è¡Œæ•°: {len(df)}, è·³è¿‡: {skipped_rows}, å¤±è´¥: {len(failures)}"
            if len(df) > 0:
                error_msg += f"\nåˆ—å: {list(df.columns)}"
                error_msg += f"\nç¬¬ä¸€è¡Œæ•°æ®: {dict(df.iloc[0]) if len(df) > 0 else 'N/A'}"
            raise HTTPException(status_code=400, detail=error_msg)
        
        # è°ƒç”¨ .NET åç«¯æ‰¹é‡åˆ›å»ºæ¥å£
        logger.info(f"å‡†å¤‡è°ƒç”¨ .NET åç«¯: {DOTNET_BASE_URL}/api/ai/kb/articles/batch, æ–‡ç« æ•°é‡: {len(articles)}")
        
        async with httpx.AsyncClient() as client:
            try:
                response = await client.post(
                    f"{DOTNET_BASE_URL}/api/ai/kb/articles/batch",
                    json={"articles": articles},
                    headers={
                        "Content-Type": "application/json",
                        "X-Tenant-Id": DEFAULT_TENANT,
                        "X-Internal-Token": INTERNAL_TOKEN
                    },
                    timeout=30.0
                )
                
                logger.info(f".NET åç«¯å“åº”çŠ¶æ€ç : {response.status_code}")
                
                if response.status_code != 200:
                    error_text = response.text
                    logger.error(f".NET åç«¯è¿”å›é”™è¯¯: {error_text}")
                    raise HTTPException(
                        status_code=response.status_code,
                        detail=f".NET åç«¯è¿”å›é”™è¯¯: {error_text}"
                    )
            except httpx.ConnectError as e:
                logger.error(f"æ— æ³•è¿æ¥åˆ° .NET åç«¯ {DOTNET_BASE_URL}: {str(e)}")
                raise HTTPException(
                    status_code=503,
                    detail=f"æ— æ³•è¿æ¥åˆ° .NET åç«¯æœåŠ¡ï¼Œè¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨: {DOTNET_BASE_URL}"
                )
            except httpx.TimeoutException as e:
                logger.error(f"è°ƒç”¨ .NET åç«¯è¶…æ—¶: {str(e)}")
                raise HTTPException(
                    status_code=504,
                    detail="è°ƒç”¨ .NET åç«¯è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
                )
            
            result = response.json()
            
            # å¤„ç†è¿”å›ç»“æœ
            success_count = result.get("successCount", 0)
            failure_count = result.get("failureCount", 0)
            article_ids = []
            article_id_to_attachment = {}  # æ˜ å°„ article_id -> attachment_info
            
            # æ”¶é›†æˆåŠŸåˆ›å»ºçš„ article IDs å’Œé™„ä»¶ä¿¡æ¯
            # æ”¯æŒä¸€ä¸ªæ–‡ç« æœ‰å¤šä¸ªé™„ä»¶ï¼ˆ_attachment_info å¯èƒ½æ˜¯å•ä¸ªå¯¹è±¡æˆ–åˆ—è¡¨ï¼‰
            article_id_to_attachments = {}  # æ”¹ä¸ºæ˜ å°„åˆ°é™„ä»¶åˆ—è¡¨
            for i, item in enumerate(result.get("results", [])):
                if item.get("success") and item.get("articleId"):
                    article_id = item["articleId"]
                    article_ids.append(article_id)
                    
                    # å¦‚æœå¯¹åº”çš„ article æœ‰é™„ä»¶ä¿¡æ¯ï¼Œä¿å­˜æ˜ å°„
                    if i < len(articles_with_attachments):
                        att_info = articles_with_attachments[i].get("_attachment_info")
                        if att_info:
                            # _attachment_info ç°åœ¨å§‹ç»ˆæ˜¯åˆ—è¡¨ï¼ˆç»Ÿä¸€å¤„ç†ï¼‰
                            if isinstance(att_info, list):
                                article_id_to_attachments[article_id] = att_info
                            else:
                                # å…¼å®¹æ—§æ•°æ®ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‡ºç°ï¼Œä½†ä¿é™©èµ·è§ï¼‰
                                article_id_to_attachments[article_id] = [att_info]
                elif not item.get("success"):
                    # è®°å½•åç«¯è¿”å›çš„å¤±è´¥ä¿¡æ¯
                    failures.append({
                        "row_index": item.get("index", -1) + (header_row + 2),  # è½¬æ¢ä¸º Excel è¡Œå·
                        "reason": item.get("error", "æœªçŸ¥é”™è¯¯")
                    })
            
            # æ‰¹é‡åˆ›å»ºé™„ä»¶ï¼ˆæ–¹æ¡ˆ Bï¼šå…ƒæ•°æ®å…³è”ï¼‰
            if article_id_to_attachments:
                total_attachments = sum(len(atts) for atts in article_id_to_attachments.values())
                logger.info(f"å‡†å¤‡åˆ›å»º {total_attachments} ä¸ªé™„ä»¶è®°å½•ï¼ˆæ¶‰åŠ {len(article_id_to_attachments)} ç¯‡æ–‡ç« ï¼‰")
                
                # è®°å½•æ¯ä¸ªæ–‡ç« çš„é™„ä»¶æ•°é‡
                for article_id, att_info_list in article_id_to_attachments.items():
                    logger.info(f"  æ–‡ç«  ID {article_id}: {len(att_info_list)} ä¸ªé™„ä»¶")
                
                assets_to_create = []
                
                for article_id, att_info_list in article_id_to_attachments.items():
                    # ä¸ºæ¯ä¸ªé™„ä»¶åˆ›å»ºè®°å½•
                    for att_info in att_info_list:
                        assets_to_create.append({
                            "articleId": article_id,
                            "assetType": att_info["asset_type"],
                            "fileName": att_info["file_name"],
                            "url": att_info["url"],
                            "size": att_info["size"],
                            "duration": None  # è§†é¢‘æ—¶é•¿æš‚ä¸æ”¯æŒè‡ªåŠ¨è·å–
                        })
                
                if assets_to_create:
                    try:
                        asset_response = await client.post(
                            f"{DOTNET_BASE_URL}/api/ai/kb/articles/assets/batch",
                            json={"assets": assets_to_create},
                            headers={
                                "Content-Type": "application/json",
                                "X-Tenant-Id": DEFAULT_TENANT,
                                "X-Internal-Token": INTERNAL_TOKEN
                            },
                            timeout=30.0
                        )
                        
                        if asset_response.status_code == 200:
                            asset_result = asset_response.json()
                            success_count = asset_result.get('successCount', 0)
                            failure_count = asset_result.get('failureCount', 0)
                            logger.info(f"âœ… é™„ä»¶åˆ›å»ºç»“æœ: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failure_count} ä¸ª")
                            
                            # è®°å½•æˆåŠŸçš„é™„ä»¶è¯¦æƒ…ï¼ˆåŒ…å«æ•°æ®åº“IDï¼‰
                            if success_count > 0:
                                results = asset_result.get('results', [])
                                created_assets = []  # è®°å½•æˆåŠŸåˆ›å»ºçš„é™„ä»¶ä¿¡æ¯
                                for result_item in results:
                                    if result_item.get('success'):
                                        index = result_item.get('index', -1)
                                        asset_id = result_item.get('assetId')
                                        if index < len(assets_to_create):
                                            success_asset = assets_to_create[index]
                                            created_assets.append({
                                                'articleId': success_asset.get('articleId'),
                                                'fileName': success_asset.get('fileName'),
                                                'assetId': asset_id
                                            })
                                            logger.info(f"  âœ“ é™„ä»¶ [{index}] å·²å†™å…¥æ•°æ®åº“: ArticleId={success_asset.get('articleId')}, FileName={success_asset.get('fileName')}, AssetId={asset_id}")
                                
                                # éªŒè¯ï¼šå°è¯•æŸ¥è¯¢åˆšåˆ›å»ºçš„é™„ä»¶ï¼ˆå¯é€‰ï¼Œç”¨äºç¡®è®¤ï¼‰
                                if created_assets:
                                    logger.info(f"ğŸ“‹ é™„ä»¶è®°å½•å·²æˆåŠŸå†™å…¥æ•°æ®åº“ï¼Œå…± {len(created_assets)} æ¡è®°å½•")
                            
                            # è®°å½•å¤±è´¥çš„é™„ä»¶è¯¦æƒ…
                            if failure_count > 0:
                                results = asset_result.get('results', [])
                                for result_item in results:
                                    if not result_item.get('success'):
                                        index = result_item.get('index', -1)
                                        error = result_item.get('error', 'æœªçŸ¥é”™è¯¯')
                                        if index < len(assets_to_create):
                                            failed_asset = assets_to_create[index]
                                            logger.warning(f"  âœ— é™„ä»¶åˆ›å»ºå¤±è´¥ [{index}]: ArticleId={failed_asset.get('articleId')}, FileName={failed_asset.get('fileName')}, Error={error}")
                        else:
                            logger.error(f"âŒ åˆ›å»ºé™„ä»¶APIè°ƒç”¨å¤±è´¥: {asset_response.status_code} - {asset_response.text}")
                            logger.error(f"   å“åº”å†…å®¹: {asset_response.text[:500]}")
                    except Exception as e:
                        logger.error(f"åˆ›å»ºé™„ä»¶æ—¶å‡ºé”™: {str(e)}")
                        # é™„ä»¶åˆ›å»ºå¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            
            # åˆå¹¶å‰ç«¯è§£æå¤±è´¥å’Œåç«¯åˆ›å»ºå¤±è´¥
            total_failures = len(failures) + failure_count
            
            return ExcelImportResponse(
                total_rows=len(df),
                success_count=success_count,
                failure_count=total_failures,
                article_ids=article_ids,
                failures=failures
            )
    
    except pd.errors.EmptyDataError:
        logger.error("Excel æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
        raise HTTPException(status_code=400, detail="Excel æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
    except httpx.HTTPError as e:
        logger.error(f"è°ƒç”¨ .NET åç«¯å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=500,
            detail=f"è°ƒç”¨ .NET åç«¯å¤±è´¥: {str(e)}"
        )
    except Exception as e:
        logger.error(f"å¯¼å…¥å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=500,
            detail=f"å¯¼å…¥å¤±è´¥: {str(e)}"
        )


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "service": "ai-hub-ai"}


if __name__ == "__main__":
    import uvicorn
    import os
    port = int(os.getenv("PORT", "8000"))
    uvicorn.run(app, host="0.0.0.0", port=port)
